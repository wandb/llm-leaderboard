services:
  vllm:
    profiles: ["vllm-docker"]
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        VLLM_VERSION: ${VLLM_VERSION:-latest}
    container_name: llm-stack-vllm-1
    runtime: nvidia
    working_dir: /workspace
    environment:
      - HF_TOKEN=${HF_TOKEN:-${HUGGINFACE_HUB_TOKEN}}
      - LOCAL_MODEL_PATH=${LOCAL_MODEL_PATH}
      - EVAL_CONFIG_PATH=${EVAL_CONFIG_PATH}
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./docker-entrypoint-vllm.sh:/workspace/docker-entrypoint-vllm.sh:ro
      - ./configs:/workspace/configs:ro
      - ./chat_templates:/workspace/chat_templates:ro
      - ${HOST_HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
      - ./.cache/vllm:/root/.cache/vllm
      - ${LOCAL_MODEL_PATH:-/tmp/empty}:/workspace/models:ro
    ports:
      - "${VLLM_PORT:-8000}:8000"
    ipc: host
    entrypoint: ["/workspace/docker-entrypoint-vllm.sh"]

  llm-leaderboard:
    build:
      context: .
      dockerfile: Dockerfile
    runtime: nvidia
    container_name: llm-leaderboard
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # Required API keys - set these in your .env file
      - WANDB_API_KEY=${WANDB_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Chakoshi moderation
      - CHAKOSHI_API_KEY=${CHAKOSHI_API_KEY}
      - CHAKOSHI_CATEGORY_SET_ID=${CHAKOSHI_CATEGORY_SET_ID}

      # Other API Keys (optional - set as needed)
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - COHERE_API_KEY=${COHERE_API_KEY}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - UPSTAGE_API_KEY=${UPSTAGE_API_KEY}
      - XAI_API_KEY=${XAI_API_KEY}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - HF_TOKEN=${HF_TOKEN:-${HUGGINFACE_HUB_TOKEN}}
      
      # VLLM Configuration
      - VLLM_ENDPOINT=vllm
      - VLLM_PORT=8000
      - VLLM_API_KEY=${VLLM_API_KEY:-EMPTY} # 空文字列を入れるとリクエストエラーになる
      - OPENAI_COMPATIBLE_API_KEY=${OPENAI_COMPATIBLE_API_KEY}

      # AWS Configuration (optional)
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}

      # Azure OpenAI (if using)
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      - OPENAI_API_TYPE=${OPENAI_API_TYPE}

      # Other settings
      - LANG=${LANG:-ja_JP.UTF-8}
      - PYTHONPATH=${PYTHONPATH:-/workspace}
      - JUMANPP_COMMAND=${JUMANPP_COMMAND:-/usr/local/bin/jumanpp}
      - WEAVE_PRINT_CALL_LINK=${WEAVE_PRINT_CALL_LINK:-false}
      #- WANDB_RUN_ID=${WANDB_RUN_ID}

      # Docker in Docker settings for SWE-bench
      - DOCKER_HOST=unix:///var/run/docker.sock

      # Docker Hub authentication for rate limit mitigation
      - DOCKER_USERNAME=${DOCKER_USERNAME}
      - DOCKER_PASSWORD=${DOCKER_PASSWORD}

      # SWE-Bench API server key (propagate into container env)
      - SWE_API_KEY=${SWE_API_KEY}

      # Dify Sandbox configuration
      - CODE_EXECUTION_ENDPOINT=${CODE_EXECUTION_ENDPOINT:-http://dify-sandbox:8194}
      - CODE_EXECUTION_API_KEY=${CODE_EXECUTION_API_KEY:-dify-sandbox}
    ipc: host
    volumes:
      - ${HOST_HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
      - ./configs:/workspace/configs:ro
      - ./blend_run_configs:/workspace/blend_run_configs:ro
      - ./chat_templates:/workspace/chat_templates:ro
      - ${LOCAL_MODEL_PATH:-/tmp/empty}:/workspace/models:ro
      - ./blend_run_configs:/workspace/blend_run_configs:ro
      
      # Docker socket mount for SWE-bench evaluation
      - /var/run/docker.sock:/var/run/docker.sock
      
      # Docker認証情報をマウント（Docker Hub rate limit対策）
      - ~/.docker/config.json:/root/.docker/config.json:ro
      
      # ローカルのスクリプトをマウント（開発用）
      - ./scripts:/workspace/scripts:rw
      - ./docker-entrypoint-vllm.sh:/workspace/docker-entrypoint-vllm.sh:ro

    command: >
      uv run scripts/run_eval.py
      -c ${EVAL_CONFIG_PATH}
    depends_on:
      - dify-sandbox
    networks:
      - default
      - ssrf_proxy_network
    dns:
      - 8.8.8.8
      - 8.8.4.4

  # Dify Sandbox service for secure code execution
  dify-sandbox:
    image: langgenius/dify-sandbox:0.2.1
    container_name: dify-sandbox
    restart: unless-stopped
    environment:
      # Sandbox configuration
      - API_KEY=${CODE_EXECUTION_API_KEY:-dify-sandbox}
      - GIN_MODE=release
      - WORKER_TIMEOUT=15
      - ENABLE_NETWORK=true
      - HTTP_PROXY=http://ssrf-proxy:3128
      - HTTPS_PROXY=http://ssrf-proxy:3128
      - SANDBOX_PORT=8194
    volumes:
      # Mount sandbox dependencies directory using a named volume
      - ./volumes/sandbox/dependencies:/dependencies
    ports:
      - "8194:8194"
    networks:
      - ssrf_proxy_network
      - default
    depends_on:
      - ssrf-proxy

  # SSRF Proxy server for secure network access
  ssrf-proxy:
    image: ubuntu/squid:latest
    container_name: ssrf-proxy
    restart: unless-stopped
    volumes:
      # Squid configuration file
      - ./configs/squid.conf:/etc/squid/squid.conf:ro
    networks:
      - ssrf_proxy_network
      - default

networks:
  default:
    name: llm-stack-network
    external: true
  ssrf_proxy_network:
    driver: bridge
    internal: false