services:
  vllm:
    profiles: ["vllm-docker"]  # vllm-dockerモードでのみ起動
    # image: vllm/vllm-openai:latest
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        VLLM_VERSION: ${VLLM_VERSION:-latest}
    environment:
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN}
      - LOCAL_MODEL_PATH=${LOCAL_MODEL_PATH}
      - EVAL_CONFIG_PATH=${EVAL_CONFIG_PATH}
    volumes:
      - ./configs:/workspace/configs:ro
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ${LOCAL_MODEL_PATH:-/tmp/empty}:/workspace/models:ro
    ports:
      - "${VLLM_PORT:-8000}:8000"
    ipc: host

  llm-leaderboard:
    build:
      context: .
      dockerfile: Dockerfile
    runtime: nvidia
    container_name: llm-leaderboard
    # GPU設定はdocker-compose.override.ymlで定義
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # Required API keys - set these in your .env file
      - WANDB_API_KEY=${WANDB_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}

      # Other API Keys (optional - set as needed)
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - COHERE_API_KEY=${COHERE_API_KEY}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - UPSTAGE_API_KEY=${UPSTAGE_API_KEY}
      - XAI_API_KEY=${XAI_API_KEY}

      # AWS Configuration (optional)
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}

      # Azure OpenAI (if using)
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      - OPENAI_API_TYPE=${OPENAI_API_TYPE}

      # Other settings
      - LANG=${LANG:-ja_JP.UTF-8}
      - PYTHONPATH=${PYTHONPATH}
      - JUMANPP_COMMAND=${JUMANPP_COMMAND}

      # Docker in Docker settings for SWE-bench
      - DOCKER_HOST=unix:///var/run/docker.sock

      # Dify Sandbox configuration
      - CODE_EXECUTION_ENDPOINT=${CODE_EXECUTION_ENDPOINT:-http://dify-sandbox:8194}
      - CODE_EXECUTION_API_KEY=${CODE_EXECUTION_API_KEY:-dify-sandbox}
    ipc: host
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./configs:/workspace/configs:ro
      - ${LOCAL_MODEL_PATH:-/tmp/empty}:/workspace/models:ro
      
      # Docker socket mount for SWE-bench evaluation
      - /var/run/docker.sock:/var/run/docker.sock
      
      # ローカルのスクリプトをマウント（開発用）
      - ./scripts:/workspace/scripts:rw

    command: >
      uv run scripts/run_eval.py
      -c ${EVAL_CONFIG_PATH}
    depends_on:
      - dify-sandbox
    networks:
      - default
      - ssrf_proxy_network
    dns:
      - 8.8.8.8
      - 8.8.4.4

  # Dify Sandbox service for secure code execution
  dify-sandbox:
    image: langgenius/dify-sandbox:0.2.1
    container_name: dify-sandbox
    restart: unless-stopped
    environment:
      # Sandbox configuration
      - API_KEY=${CODE_EXECUTION_API_KEY:-dify-sandbox}
      - GIN_MODE=release
      - WORKER_TIMEOUT=15
      - ENABLE_NETWORK=true
      - HTTP_PROXY=http://ssrf-proxy:3128
      - HTTPS_PROXY=http://ssrf-proxy:3128
      - SANDBOX_PORT=8194
    volumes:
      # Mount sandbox dependencies directory
      - ./volumes/sandbox/dependencies:/dependencies
    ports:
      - "8194:8194"
    networks:
      - ssrf_proxy_network
      - default
    depends_on:
      - ssrf-proxy

  # SSRF Proxy server for secure network access
  ssrf-proxy:
    image: ubuntu/squid:latest
    container_name: ssrf-proxy
    restart: unless-stopped
    volumes:
      # Squid configuration file
      - ./configs/squid.conf:/etc/squid/squid.conf:ro
    networks:
      - ssrf_proxy_network
      - default

networks:
  default:
    name: llm-stack-network
    external: true
  ssrf_proxy_network:
    driver: bridge
    internal: false