#!/bin/bash
set -e

# ä½¿ã„æ–¹ã®è¡¨ç¤º
show_usage() {
    echo "ä½¿ã„æ–¹: $0 <ãƒ¢ãƒ‡ãƒ«åã¾ãŸã¯configãƒ•ã‚¡ã‚¤ãƒ«å>"
    echo ""
    echo "ä¾‹:"
    echo "  $0 Meta-Llama-3-8B-Instruct"
    echo "  $0 config-Meta-Llama-3-8B-Instruct.yaml"
    echo "  $0 Swallow-7b"  # éƒ¨åˆ†ä¸€è‡´ã‚‚å¯èƒ½
    echo ""
    echo "ã‚ªãƒ—ã‚·ãƒ§ãƒ³:"
    echo "  -h, --help     ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º"
    echo "  -l, --list     åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä¸€è¦§è¡¨ç¤º"
    echo "  -o, --offline  Wandbã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ"
    echo "  -d, --debug    ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ"
}

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º
list_models() {
    echo "åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:"
    echo ""
    for config in configs/config-*.yaml; do
        if [ -f "$config" ]; then
            # YAMLã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
            model_name=$(grep "pretrained_model_name_or_path:" "$config" | head -1 | sed 's/.*pretrained_model_name_or_path: *//' | sed 's/ *#.*//' | tr -d '"' | tr -d "'")
            config_name=$(basename "$config" .yaml | sed 's/config-//')
            printf "  %-40s â†’ %s\n" "$config_name" "$model_name"
        fi
    done
}

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
find_config() {
    local query=$1
    
    # å®Œå…¨ä¸€è‡´ã‚’è©¦ã™
    if [ -f "configs/config-${query}.yaml" ]; then
        echo "configs/config-${query}.yaml"
        return 0
    fi
    
    # .yamlä»˜ãã§æ¸¡ã•ã‚ŒãŸå ´åˆ
    if [ -f "configs/${query}" ]; then
        echo "configs/${query}"
        return 0
    fi
    
    # éƒ¨åˆ†ä¸€è‡´ã‚’è©¦ã™
    local matches=(configs/config-*${query}*.yaml)
    if [ -f "${matches[0]}" ]; then
        if [ ${#matches[@]} -eq 1 ]; then
            echo "${matches[0]}"
            return 0
        else
            echo "ã‚¨ãƒ©ãƒ¼: è¤‡æ•°ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:" >&2
            for match in "${matches[@]}"; do
                echo "  - $match" >&2
            done
            return 1
        fi
    fi
    
    echo "ã‚¨ãƒ©ãƒ¼: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: $query" >&2
    return 1
}

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
main() {
    local OFFLINE_MODE=false
    local DEBUG_MODE=false
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è§£æ
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                show_usage
                exit 0
                ;;
            -l|--list)
                list_models
                exit 0
                ;;
            -o|--offline)
                OFFLINE_MODE=true
                shift
                ;;
            -d|--debug)
                DEBUG_MODE=true
                shift
                ;;
            *)
                MODEL_QUERY=$1
                shift
                ;;
        esac
    done
    
    # ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
    if [ -z "$MODEL_QUERY" ]; then
        show_usage
        exit 1
    fi
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    CONFIG_FILE=$(find_config "$MODEL_QUERY")
    if [ $? -ne 0 ]; then
        exit 1
    fi
    
    echo "ğŸ“‹ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: $CONFIG_FILE"
    
    # YAMLã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’æŠ½å‡º
    MODEL_NAME=$(grep "pretrained_model_name_or_path:" "$CONFIG_FILE" | head -1 | sed 's/.*pretrained_model_name_or_path: *//' | sed 's/ *#.*//' | tr -d '"' | tr -d "'" | xargs)
    echo "ğŸ¤– ãƒ¢ãƒ‡ãƒ«: $MODEL_NAME"
    
    # ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
    export EVAL_CONFIG_PATH=$(basename "$CONFIG_FILE")
    export VLLM_MODEL_NAME="$MODEL_NAME"
    
    if [ "$OFFLINE_MODE" = true ]; then
        export WANDB_MODE=offline
        echo "ğŸ“Š Wandb: ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰"
    else
        export WANDB_MODE=online
    fi
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    if [ "$DEBUG_MODE" = true ]; then
        echo ""
        echo "ãƒ‡ãƒãƒƒã‚°æƒ…å ±:"
        echo "  EVAL_CONFIG_PATH=$EVAL_CONFIG_PATH"
        echo "  VLLM_MODEL_NAME=$VLLM_MODEL_NAME"
        echo "  WANDB_MODE=$WANDB_MODE"
        echo ""
    fi
    
    # Dockerã‚³ãƒ³ãƒ†ãƒŠã®ç¢ºèªã¨èµ·å‹•
    if ! docker ps | grep -q llm-leaderboard; then
        echo "ğŸš€ è©•ä¾¡ã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•ä¸­..."
        docker run -d --name llm-leaderboard --gpus all \
            --network llm-stack-network \
            -e EVAL_CONFIG_PATH="$EVAL_CONFIG_PATH" \
            -e VLLM_MODEL_NAME="$VLLM_MODEL_NAME" \
            -e WANDB_MODE="$WANDB_MODE" \
            -e NVIDIA_VISIBLE_DEVICES=all \
            -e DOCKER_HOST=unix:///var/run/docker.sock \
            -e CODE_EXECUTION_API_KEY=dify-sandbox \
            -e CODE_EXECUTION_ENDPOINT=http://dify-sandbox:8194 \
            -v ~/.cache/huggingface:/root/.cache/huggingface \
            -v ./configs:/workspace/configs:ro \
            -v ./scripts:/workspace/scripts:rw \
            -v /var/run/docker.sock:/var/run/docker.sock \
            --ipc=host \
            llm-stack-llm-leaderboard:latest \
            tail -f /dev/null
        
        # ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•ã‚’å¾…ã¤
        sleep 3
    fi
    
    # è©•ä¾¡ã®å®Ÿè¡Œ
    echo ""
    echo "ğŸ”¬ è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™..."
    echo "=========================================="
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ­ã‚°ã‚’è¡¨ç¤ºã—ãªãŒã‚‰å®Ÿè¡Œ
    docker exec -it llm-leaderboard bash -c "
        cd /workspace && 
        source .venv/bin/activate && 
        export WANDB_MODE=$WANDB_MODE &&
        python3 scripts/run_eval.py --config $EVAL_CONFIG_PATH
    "
    
    echo "=========================================="
    echo "âœ… è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸ"
}

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
main "$@" 