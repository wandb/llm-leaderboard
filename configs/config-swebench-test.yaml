wandb:
  entity: llm-leaderboard
  project: nejumi-leaderboard4
  run_name: gpt-4.1-2025-04-14 (SWE-Bench Verified Test)
testmode: false
run:
  jaster: false
  jmmlu_robustness: false
  mtbench: false
  jbbq: false
  lctg: false
  toxicity: false
  jtruthfulqa: false
  swebench: true
  aggregate: true
api: openai
batch_size: 4
inference_interval: 1
model:
  pretrained_model_name_or_path: gpt-4.1-2025-04-14 #if you use openai api, put the name of model
  bfcl_model_name: "gpt-4.1-2025-04-14-FC" # check the model name in "llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/SUPPORTED_MODELS.md"
  size_category: api
  size: null
  release_date: 4/14/2025
generator:
  top_p: 1.0
  temperature: 0.1
  max_tokens: 32768
swebench:
  artifacts_path: llm-leaderboard/nejumi-leaderboard4/swebench_verified_official:v0
  dataset_dir: swebench_verified_official
  max_samples: 500
  max_tokens: 32768
  max_workers: 32
  evaluation_method: official
  upload_data: false
