wandb:
  entity: llm-leaderboard
  project: nejumi-leaderboard4
  run_name: swebench-full-run
testmode: false
run:
  jaster: false
  jmmlu_robustness: false
  mtbench: false
  jbbq: false
  lctg: false
  toxicity: false
  jtruthfulqa: false
  swebench: true
  aggregate: true
api: openai
batch_size: 16
inference_interval: 2
model:
  pretrained_model_name_or_path: o3 #if you use openai api, put the name of model
  bfcl_model_name: "" # check the model name in "llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/SUPPORTED_MODELS.md"
  size_category: api
  size: null
  release_date: 5/13/2024
generator:
  top_p: 1.0
  temperature: 0.1
  max_tokens: 4096
swebench:
  artifacts_path: wandb-japan/llm-leaderboard3/swebench_verified:v1
  dataset_dir: swebench
  max_samples: 500
  max_tokens: 4096
  max_workers: 8
  evaluation_method: official
