wandb:
  entity: 'llm-leaderboard'
  project: 'nejumi-leaderboard4'
  #run: please set up run name in a model-base config

testmode: false
inference_interval: 0 # seconds

run:
  jaster: true
  jmmlu_robustness: true # if this is set as true, jaster should set as true
  mtbench: true
  jbbq: true
  toxicity: true
  jtruthfulqa: true
  hle: true
  swebench: true
  bfcl: true
  hallulens: true
  arc_agi: true
  m_ifeval: true
  aggregate: true

model:
  artifact_path: null

vllm: # vLLM server-side launch configuration
  # vLLM docker image tag (default). Override in each model config with `vllm_tag:` or `vllm_docker_image:`
  vllm_tag: latest
  # Volta/Turing など Ampere 未満の GPU で vLLM が Triton MMA カーネルを使ってクラッシュする場合に true に設定
  disable_triton_mma: false
  # vLLMコンテナのライフサイクル管理: auto | always_on | stop_restart
  lifecycle: auto
  # モデルのデータ型 null の場合はモデル内config.jsonのデフォルト値を使用
  dtype: null
  # モデルの最大トークン数 null の場合はモデル内config.jsonのデフォルト値を使用
  max_model_len: null
  # モデルのデバイスマップ
  device_map: null
  # GPUメモリ使用率制限
  gpu_memory_utilization: null
  # Reasoningモデル用のParser設定
  reasoning_parser: null
  # HF repository内のリモートコードを信頼するかどうか
  trust_remote_code: false
  # Tool callingの自動パースの有効化
  enable_auto_tool_choice: false
  # ツール呼び出しのパーサー
  tool_call_parser: null
  # 追加のvLLMコマンドライン引数
  # extra_args:
  #   - '--override_generation_config={"temperature":0.6,"top_p":0.95}' # do not include space
  extra_args: []

generator: # LLM client default configuration: to be overridden by each benchmark
  top_p: 1.0
  temperature: 0.1
  max_tokens: 128

num_few_shots: 2

jaster:
  message_intro: '以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。'
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/jaster:production'
  dataset_dir: 'jaster'
  jhumaneval:
    # Dify Sandbox configuration for secure code execution
    dify_sandbox:
      endpoint: 'http://dify-sandbox:8194'  # Sandbox service endpoint
      api_key: 'dify-sandbox'               # API key for authentication
      connect_timeout: 10.0                 # Connection timeout in seconds
      read_timeout: 60.0                    # Read timeout in seconds  
      write_timeout: 60.0                   # Write timeout in seconds
      enable_network: true                  # Enable network access in sandbox
      worker_timeout: 15                    # Worker process timeout in seconds
  override_max_tokens: null # if null, use value defined in task dataset files

jbbq:
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4-private/jbbq:production'
  dataset_dir: 'jbbq'
  language: 'ja'
  generator_config:
    max_tokens: 10

toxicity:
  artifact_path: 'llm-leaderboard/nejumi-leaderboard4-private/toxicity_dataset_full:production'
  judge_prompts_path: 'llm-leaderboard/nejumi-leaderboard4-private/toxicity_judge_prompts:production'
  generator_config:
    max_tokens: 256
    temperature: 0.0
    top_p: 1.0
  judge:
    model: 'gpt-4.1-2025-04-14'
    parallel: 32
    params: {} # Currently using OpenAI's default

jtruthfulqa:
  artifact_path: 'llm-leaderboard/nejumi-leaderboard4/jtruthfulqa_dataset:production'  # JTruthfulQAデータセットのアーティファクトパス
  roberta_model_name: 'nlp-waseda/roberta_jtruthfulqa'  # 評価に使用するRoBERTaモデル名
  generator_config:
    max_tokens: 256

swebench:
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/swebench_verified_official:production'  # SWE-Bench Verifiedデータセットのアーティファクトパス
  dataset_dir: 'swebench_verified_official'
  max_samples: 80  # 全500件は時間がかかるため制限（フルセットの場合は500を指定）
  max_tokens: 2048  # SWE-Benchのパッチ生成に必要なトークン数
  max_workers: 24  # 並列実行ワーカー数
  evaluation_method: 'official'  # 'official' or 'docker' - 公式パッケージまたは独自Docker実装
  background_eval: false

mtbench:
  temperature_override:
    writing: 0.7
    roleplay: 0.7
    extraction: 0.0
    math: 0.0
    coding: 0.0
    reasoning: 0.0
    stem: 0.1
    humanities: 0.1
  question_artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_question:production' # if testmode is true, small dataset will be used
  question_artifacts_path_test: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_question_small_for_test:production'
  referenceanswer_artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_referenceanswer:production' # if testmode is true, small dataset will be used
  referenceanswer_artifacts_path_test: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_referenceanswer_small_for_test:production'
  judge_prompt_artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/mtbench_ja_prompt:production' 
  bench_name: 'japanese_mt_bench'
  model_id: null # cannot use '<', '>', ':', ''', '/', '\\', '|', '?', '*', '.'
  question_begin: null 
  question_end: null 
  max_new_token: 1024
  num_choices: 1
  num_gpus_per_model: 1
  num_gpus_total: 1
  max_gpu_memory: null
  dtype: float16 # None or float32 or float16 or bfloat16
  # for gen_judgment
  judge_model: 'gpt-4o-2024-05-13'
  mode: 'single'
  baseline_model: null 
  parallel: 80
  first_n: null
  judge:
    model: 'gpt-4.1-2025-04-14'
    parallel: 32
    params:
      max_tokens: 2048
      temperature: 0.0

bfcl:
  temperature: 0.01  # 推論共通化未対応のHandler用: 移行終わったら消す
  generator_config:  # 推論共通化後のHandler用
    max_tokens: 4096
    temperature: 0.01
  num_threads: 4
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/bfcl:production'

hallulens:
  artifacts_path: 'llm-leaderboard/nejumi-leaderboard4/hallulens:production'
  generator_config:
    max_tokens: 256
    temperature: 0.0
    top_p: 1.0
  judge:
    model: 'gpt-4.1-2025-04-14'
    parallel: 32
    params: {} # Currently using OpenAI's default

hle:
  artifact_path: 'llm-leaderboard/nejumi-leaderboard4/hle-ja:production'
  dataset_name: 'hle-ja.jsonl'
  max_completion_tokens: 4096
  max_samples: 10
  judge:
    model: "o3-mini-2025-01-31"
    parallel: 32
    params: {} # Currently using OpenAI's default

arc_agi:
  arc_agi_1_artifacts_path: "llm-leaderboard/nejumi-leaderboard4/arc-agi-1_public-eval_50:production"
  arc_agi_2_artifacts_path: "llm-leaderboard/nejumi-leaderboard4/arc-agi-2_public-eval_50:production"
  max_output_tokens: 4096
  num_attempts: 2

m_ifeval:
  artifacts_path: "llm-leaderboard/nejumi-leaderboard4/m_ifeval:production"
  dataset_dir: "ja_input_data.jsonl"

sample_dataset:
  artifacts_path: "your artifact path here"
  # add necessary configration here
