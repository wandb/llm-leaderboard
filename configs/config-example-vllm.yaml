wandb:
  run_name: "example-vllm-swallow-7b"

api: "vllm"  # 最もシンプルな指定（推奨）
# base_urlは自動的に "http://vllm:8000/v1" が設定されます

model:
  pretrained_model_name_or_path: "tokyotech-llm/Swallow-7b-instruct-v0.1"
  dtype: 'float16'
  max_model_len: 3000
  trust_remote_code: true

# vLLM server configuration
vllm:
  extra_args:
    - --max-model-len=3000
    - --trust-remote-code

# Number of GPUs for tensor parallelism
num_gpus: 1 