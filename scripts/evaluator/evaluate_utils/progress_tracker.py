"""
è©•ä¾¡ãƒãƒ¼ãƒã‚¹ã®é€²è¡ŒçŠ¶æ³ã¨ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ã®ç¾åŒ–æ©Ÿèƒ½
"""
import time
import wandb
from typing import Dict, List, Optional, Any
import json


class EvaluationProgressTracker:
    """è©•ä¾¡é€²è¡ŒçŠ¶æ³ã®è¿½è·¡ã¨ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›"""
    
    def __init__(self, enabled_benchmarks: List[str]):
        self.enabled_benchmarks = enabled_benchmarks
        self.completed_benchmarks = []
        self.current_benchmark = None
        self.benchmark_results = {}
        self.start_time = time.time()
        
    def start_tracking(self):
        """é€²è¡ŒçŠ¶æ³ã®è¿½è·¡ã‚’é–‹å§‹"""
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º
        self._show_header()
        
    def start_benchmark(self, benchmark_name: str):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹"""
        self.current_benchmark = benchmark_name
        self._show_benchmark_start(benchmark_name)
        
    def update_benchmark_progress(self, progress_percent: int):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é€²è¡ŒçŠ¶æ³ã‚’æ›´æ–°"""
        # ã‚·ãƒ³ãƒ—ãƒ«å®Ÿè£…ã§ã¯ä½•ã‚‚ã—ãªã„
        pass
            
    def complete_benchmark(self, benchmark_name: str, results: Optional[Dict[str, Any]] = None):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†"""
        self.completed_benchmarks.append(benchmark_name)
        
        if results:
            self.benchmark_results[benchmark_name] = results
            
        # çµæœè¡¨ç¤º
        self._show_benchmark_completion(benchmark_name, results)
        
    def show_leaderboard_table(self, benchmark_name: str, wandb_run: Optional[Any] = None):
        """W&Bã‹ã‚‰ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—ã—ã¦è¡¨ç¤º"""
        try:
            if wandb_run is None:
                wandb_run = wandb.run
                
            if wandb_run is None:
                print(f"âš ï¸  W&B run not available for {benchmark_name}")
                return
                
            # W&Bã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
            table_key = f"{benchmark_name}_leaderboard_table"
            
            # W&B APIã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
            api = wandb.Api()
            run = api.run(f"{wandb_run.entity}/{wandb_run.project}/{wandb_run.id}")
            
            # ãƒ­ã‚°ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
            for log_entry in run.scan_history(keys=[table_key]):
                if table_key in log_entry:
                    table_data = log_entry[table_key]
                    self._render_leaderboard_table(benchmark_name, table_data)
                    break
            else:
                print(f"âš ï¸  No leaderboard table found for {benchmark_name}")
                
        except Exception as e:
            print(f"âŒ Error displaying leaderboard for {benchmark_name}: {e}")
            
    def _render_leaderboard_table(self, benchmark_name: str, table_data: Any):
        """ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚·ãƒ³ãƒ—ãƒ«å½¢å¼ã§è¡¨ç¤º"""
        try:
            if hasattr(table_data, 'data'):
                # W&B Tableã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                columns = table_data.columns
                data = table_data.data
            elif isinstance(table_data, dict):
                # è¾æ›¸å½¢å¼ã®å ´åˆ
                columns = table_data.get('columns', [])
                data = table_data.get('data', [])
            else:
                print(f"âš ï¸  Unknown table format for {benchmark_name}")
                return
                
            print(f"\n{'='*80}")
            print(f"ğŸ† {benchmark_name.upper()} LEADERBOARD")
            print(f"{'='*80}")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¡¨ç¤º
            header = " | ".join([f"{col:>12}" for col in columns])
            print(header)
            print("-" * len(header))
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºï¼ˆä¸Šä½10ä»¶ã®ã¿ï¼‰
            for i, row in enumerate(data[:10]):
                row_str = " | ".join([f"{str(cell):>12}" for cell in row])
                rank_indicator = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else f"{i+1:2d}"
                print(f"{rank_indicator} {row_str}")
                
            print(f"{'='*80}\n")
            
        except Exception as e:
            print(f"âŒ Error rendering table for {benchmark_name}: {e}")
            
    def _show_header(self):
        """ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸš€ LLM EVALUATION HARNESS")
        print("="*80)
        print(f"ğŸ“Š Total Benchmarks: {len(self.enabled_benchmarks)}")
        print(f"â° Started: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        enabled_list = ", ".join(self.enabled_benchmarks)
        print(f"ğŸ¯ Enabled: {enabled_list}")
        print("="*80 + "\n")
        
    def _show_benchmark_start(self, benchmark_name: str):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹ã®è¡¨ç¤º"""
        emoji = self._get_benchmark_emoji(benchmark_name)
        progress = f"{len(self.completed_benchmarks)+1}/{len(self.enabled_benchmarks)}"
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ [{progress}] STARTING {emoji} {benchmark_name.upper()}")
        print(f"{'='*60}")
        
    def _show_benchmark_completion(self, benchmark_name: str, results: Optional[Dict[str, Any]]):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ã®è¡¨ç¤º"""
        elapsed = time.time() - self.start_time
        
        print(f"\n{'='*60}")
        print(f"âœ… {benchmark_name.upper()} COMPLETED")
        print(f"{'='*60}")
        print(f"â±ï¸  Elapsed: {elapsed:.1f}s")
        
        if results:
            print("ğŸ“Š Key Metrics:")
            # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
            for key, value in list(results.items())[:5]:  # æœ€åˆã®5ã¤ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                if isinstance(value, (int, float)):
                    print(f"   {key}: {value:.3f}")
                    
        remaining = len(self.enabled_benchmarks) - len(self.completed_benchmarks)
        print(f"ğŸ“ˆ Progress: {len(self.completed_benchmarks)}/{len(self.enabled_benchmarks)} ({remaining} remaining)")
        print(f"{'='*60}\n")
        
    def finish_tracking(self):
        """è¿½è·¡çµ‚äº†"""            
        total_elapsed = time.time() - self.start_time
        
        # æœ€çµ‚ã‚µãƒãƒªãƒ¼
        print("\n" + "="*80)
        print("ğŸ‰ EVALUATION COMPLETED!")
        print("="*80)
        print(f"â±ï¸  Total Time: {total_elapsed:.1f}s")
        print(f"âœ… Completed: {len(self.completed_benchmarks)}/{len(self.enabled_benchmarks)}")
        
        if self.benchmark_results:
            print("\nğŸ“Š Final Results Summary:")
            for benchmark, results in self.benchmark_results.items():
                if results and isinstance(results, dict):
                    # æœ€åˆã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
                    first_metric = next(iter(results.items()))
                    if isinstance(first_metric[1], (int, float)):
                        print(f"  {benchmark}: {first_metric[0]}={first_metric[1]:.3f}")
                        
        print("="*80)

    def _get_benchmark_emoji(self, benchmark_name: str) -> str:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åã‹ã‚‰ã‚·ãƒ¼ãƒ‰å›ºå®šã®ãƒ©ãƒ³ãƒ€ãƒ çµµæ–‡å­—ã‚’é¸æŠ"""
        import hashlib
        
        # è©•ä¾¡é–¢é€£ã®çµµæ–‡å­—ãƒªã‚¹ãƒˆ
        emojis = [
            'ğŸ¯', 'ğŸ”§', 'ğŸ›', 'ğŸ“Š', 'ğŸ›¡ï¸', 'âœ…', 'ğŸ§ ', 'ğŸ‘ï¸', 
            'ğŸ²', 'ğŸ“', 'ğŸŒ¸', 'ğŸ”¢', 'ğŸ’»', 'ğŸ¤”', 'ğŸ“–', 'ğŸ’¬',
            'ğŸ“„', 'ğŸŒ', 'â“', 'ğŸ‘€', 'ğŸ“', 'âš¡', 'ğŸš€', 'ğŸ’¡',
            'ğŸ”', 'â­', 'ğŸª', 'ğŸ¨', 'ğŸµ', 'ğŸ­', 'ğŸ¬', 'ğŸ®'
        ]
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ã—ã¦ã‚·ãƒ¼ãƒ‰ã¨ã—ã¦ä½¿ç”¨
        hash_value = hashlib.md5(benchmark_name.encode()).hexdigest()
        # ãƒãƒƒã‚·ãƒ¥ã®æœ€åˆã®8æ–‡å­—ã‚’16é€²æ•°ã¨ã—ã¦è§£é‡ˆã—ã€çµµæ–‡å­—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š
        seed = int(hash_value[:8], 16)
        emoji_index = seed % len(emojis)
        
        return emojis[emoji_index]


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_global_tracker: Optional[EvaluationProgressTracker] = None


def get_progress_tracker() -> Optional[EvaluationProgressTracker]:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’å–å¾—"""
    return _global_tracker


def initialize_progress_tracker(enabled_benchmarks: List[str]) -> EvaluationProgressTracker:
    """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’åˆæœŸåŒ–"""
    global _global_tracker
    _global_tracker = EvaluationProgressTracker(enabled_benchmarks)
    return _global_tracker


def start_benchmark_tracking(benchmark_name: str):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¿½è·¡é–‹å§‹"""
    if _global_tracker:
        _global_tracker.start_benchmark(benchmark_name)


def complete_benchmark_tracking(benchmark_name: str, results: Optional[Dict[str, Any]] = None):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¿½è·¡å®Œäº†"""
    if _global_tracker:
        _global_tracker.complete_benchmark(benchmark_name, results)
        _global_tracker.show_leaderboard_table(benchmark_name)


def update_benchmark_progress(progress_percent: int):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é€²è¡ŒçŠ¶æ³æ›´æ–°"""
    if _global_tracker:
        _global_tracker.update_benchmark_progress(progress_percent)


def finish_progress_tracking():
    """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¿½è·¡çµ‚äº†"""
    if _global_tracker:
        _global_tracker.finish_tracking()