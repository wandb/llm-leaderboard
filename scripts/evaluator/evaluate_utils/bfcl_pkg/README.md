# Berkeley Function Calling Leaderboard (BFCL)

> **Note**: This directory is imported from [Berkeley Function Call Leaderboard](https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard) for Nejumi Leaderboard project.
> 
> Last updated by Keisuke Kamata on 2025/05/30
>
> To update this directory with the latest version:
> ```bash
> cd /path/to/project/root
> rm -rf scripts/evaluator/evaluate_utils/bfcl
> mkdir -p berkeley-function-call-leaderboard && cd berkeley-function-call-leaderboard
> git init && git remote add origin https://github.com/ShishirPatil/gorilla.git
> git config core.sparseCheckout true
> echo "berkeley-function-call-leaderboard/*" > .git/info/sparse-checkout
> git pull origin main
> cp -r berkeley-function-call-leaderboard/* ../scripts/evaluator/evaluate_utils/bfcl/
> cd .. && rm -rf berkeley-function-call-leaderboard
> ```

## Table of Contents

- [Berkeley Function Calling Leaderboard (BFCL)](#berkeley-function-calling-leaderboard-bfcl)
  - [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Installation \& Setup](#installation--setup)
    - [Basic Installation](#basic-installation)
    - [Extra Dependencies for Self-Hosted Models](#extra-dependencies-for-self-hosted-models)
    - [Setting up Environment Variables](#setting-up-environment-variables)
  - [Running Evaluations](#running-evaluations)
    - [Generating LLM Responses](#generating-llm-responses)
      - [Selecting Models and Test Categories](#selecting-models-and-test-categories)
      - [Output and Logging](#output-and-logging)
      - [For API-based Models](#for-api-based-models)
      - [For Locally-hosted OSS Models](#for-locally-hosted-oss-models)
        - [For Pre-existing OpenAI-compatible Endpoints](#for-pre-existing-openai-compatible-endpoints)
      - [(Alternate) Script Execution for Generation](#alternate-script-execution-for-generation)
    - [Evaluating Generated Responses](#evaluating-generated-responses)
      - [Output Structure](#output-structure)
      - [(Optional) WandB Evaluation Logging](#optional-wandb-evaluation-logging)
      - [(Alternate) Script Execution for Evaluation](#alternate-script-execution-for-evaluation)
  - [Contributing \& How to Add New Models](#contributing--how-to-add-new-models)
  - [Additional Resources](#additional-resources)

---

## Introduction

We introduce the Berkeley Function Calling Leaderboard (BFCL), the **first comprehensive and executable function call evaluation** dedicated to assessing Large Language Models' (LLMs) ability to invoke functions. Unlike previous evaluations, BFCL accounts for various forms of function calls, diverse scenarios, and executability.

ğŸ’¡ Read more in our blog posts:

- [BFCL v1 (original) Blog Post](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html)
- [BFCL v2 (live dataset) Blog Post](https://gorilla.cs.berkeley.edu/blogs/12_bfcl_v2_live.html)
- [BFCL v3 (multi-turn) Blog Post](https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html)

ğŸ¦ See the live leaderboard at [Berkeley Function Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html#leaderboard)

![Architecture Diagram](./architecture_diagram.png)

---

## Installation & Setup

### Basic Installation

```bash
# Create a new Conda environment with Python 3.10
conda create -n BFCL python=3.10
conda activate BFCL

# Clone the Gorilla repository
git clone https://github.com/ShishirPatil/gorilla.git

# Change directory to the `berkeley-function-call-leaderboard`
cd gorilla/berkeley-function-call-leaderboard

# Install the package in editable mode
pip install -e .
```

### Extra Dependencies for Self-Hosted Models

For locally hosted models, choose one of the following backends, ensuring you have the right GPU and OS setup:

`sglang` is *much faster* than `vllm` but only supports newer GPUs with SM 80+ (Ampere etc).
If you are using an older GPU (T4/V100), you should use `vllm` instead as it supports a much wider range of GPUs.

**Using `vllm`:**
```bash
pip install -e .[oss_eval_vllm]
```

**Using `sglang`:**
```bash
pip install -e .[oss_eval_sglang]
```

*Optional:* If using `sglang`, we recommend installing `flashinfer` for speedups. Find instructions [here](https://docs.flashinfer.ai/installation.html).

### Setting up Environment Variables

We store environment variables in a `.env` file. We have provided a example `.env.example` file in the `gorilla/berkeley-function-call-leaderboard` directory. You should make a copy of this file, and fill in the necessary values.

```bash
cp .env.example .env
# Fill in necessary values in `.env`
```

If you are running any proprietary models, make sure the model API keys are included in your `.env` file. Models like GPT, Claude, Mistral, Gemini, Nova, will require them.

---

## Running Evaluations

### Generating LLM Responses

#### Selecting Models and Test Categories

- `MODEL_NAME`: For available models, please refer to [SUPPORTED_MODELS.md](./SUPPORTED_MODELS.md). If not specified, the default model `gorilla-openfunctions-v2` is used.
- `TEST_CATEGORY`: For available test categories, please refer to [TEST_CATEGORIES.md](./TEST_CATEGORIES.md). If not specified, all categories are included by default.

You can provide multiple models or test categories by separating them with commas. For example:

```bash
bfcl generate --model claude-3-5-sonnet-20241022-FC,gpt-4o-2024-11-20-FC --test-category simple,parallel,multiple,multi_turn
```

#### Output and Logging

- All generated model responses are stored in `./result/` folder, organized by model and test category: `result/MODEL_NAME/BFCL_v3_TEST_CATEGORY_result.json`
- To use a custom directory for the result file, specify using `--result-dir`; path should be relative to the `berkeley-function-call-leaderboard` root folder,

An inference log is included with the model responses to help analyze/debug the model's performance, and to better understand the model behavior. For more verbose logging, use the `--include-input-log` flag. Refer to [LOG_GUIDE.md](./LOG_GUIDE.md) for details on how to interpret the inference logs.

#### For API-based Models

```bash
bfcl generate --model MODEL_NAME --test-category TEST_CATEGORY --num-threads 1
```

- Use `--num-threads` to control the level of parallel inference. The default (`1`) means no parallelization.
- The maximum allowable threads depends on your API's rate limits.

#### For Locally-hosted OSS Models

```bash
bfcl generate \
  --model MODEL_NAME \
  --test-category TEST_CATEGORY \
  --backend {vllm|sglang} \
  --num-gpus 1 \
  --gpu-memory-utilization 0.9 \
  --local-model-path /path/to/local/model   # â† optional
```

- Choose your backend using `--backend vllm` or `--backend sglang`. The default backend is `vllm`.
- Control GPU usage by adjusting `--num-gpus` (default `1`, relevant for multi-GPU tensor parallelism) and `--gpu-memory-utilization` (default `0.9`), which can help avoid out-of-memory errors.
- `--local-model-path` (optional): Point this flag at a directory that already contains the model's files (`config.json`, tokenizer, weights, etc.). Use it only when you've preâ€‘downloaded the model and the weights live somewhere other than the default `$HF_HOME` cache.

##### For Pre-existing OpenAI-compatible Endpoints

If you have a server already running (e.g., vLLM in a SLURM cluster), you can bypass the vLLM/sglang setup phase and directly generate responses by using the `--skip-server-setup` flag:

```bash
bfcl generate --model MODEL_NAME --test-category TEST_CATEGORY --skip-server-setup
```

In addition, you should specify the endpoint and port used by the server. By default, the endpoint is `localhost` and the port is `1053`. These can be overridden by the `VLLM_ENDPOINT` and `VLLM_PORT` environment variables in the `.env` file:

```bash
VLLM_ENDPOINT=localhost
VLLM_PORT=1053
```

#### (Alternate) Script Execution for Generation

For those who prefer using script execution instead of the CLI, you can run the following command:

```bash
# Make sure you are inside the `berkeley-function-call-leaderboard` directory
python openfunctions_evaluation.py --model MODEL_NAME --test-category TEST_CATEGORY
```

When specifying multiple models or test categories, separate them with **spaces**, not commas. All other flags mentioned earlier are compatible with the script execution method as well.

### Evaluating Generated Responses

**Important:** You must have generated the model responses before running the evaluation.

Once you have the results, run:

```bash
bfcl evaluate --model MODEL_NAME --test-category TEST_CATEGORY
```

The `MODEL_NAME` and `TEST_CATEGORY` options are the same as those used in the [Generating LLM Responses](#generating-llm-responses) section. For details, refer to [SUPPORTED_MODELS.md](./SUPPORTED_MODELS.md) and [TEST_CATEGORIES.md](./TEST_CATEGORIES.md).

If in the previous step you stored the model responses in a custom directory, you should specify it using the `--result-dir` flag; path should be relative to the `berkeley-function-call-leaderboard` root folder.

> Note: For unevaluated test categories, they will be marked as `N/A` in the evaluation result csv files.
> For summary columns (e.g., `Overall Acc`, `Non_Live Overall Acc`, `Live Overall Acc`, and `Multi Turn Overall Acc`), the score reported will treat all unevaluated categories as 0 during calculation.

#### Output Structure

Evaluation scores are stored in `./score/`, mirroring the structure of `./result/`: `score/MODEL_NAME/BFCL_v3_TEST_CATEGORY_score.json`

- To use a custom directory for the score file, specify using `--score-dir`; path should be relative to the `berkeley-function-call-leaderboard` root folder.

Additionally, four CSV files are generated in `./score/`:

- `data_overall.csv` â€“ Overall scores for each model. This is used for updating the leaderboard.
- `data_live.csv` â€“ Detailed breakdown of scores for each Live (single-turn) test category.
- `data_non_live.csv` â€“ Detailed breakdown of scores for each Non-Live (single-turn) test category.
- `data_multi_turn.csv` â€“ Detailed breakdown of scores for each Multi-Turn test category.

#### (Optional) WandB Evaluation Logging

If you'd like to log evaluation results to WandB artifacts:

```bash
pip install -e.[wandb]
```

Mkae sure you also set `WANDB_BFCL_PROJECT=ENTITY:PROJECT` in `.env`.

#### (Alternate) Script Execution for Evaluation

For those who prefer using script execution instead of the CLI, you can run the following command:

```bash
# Make sure you are inside the `berkeley-function-call-leaderboard/bfcl/eval_checker` directory
cd bfcl/eval_checker
python eval_runner.py --model MODEL_NAME --test-category TEST_CATEGORY
```

When specifying multiple models or test categories, separate them with **spaces**, not commas. All other flags mentioned earlier are compatible with the script execution method as well.

## Contributing & How to Add New Models

We welcome contributions! To add a new model:

1. Review `bfcl/model_handler/base_handler.py` and/or `bfcl/model_handler/local_inference/base_oss_handler.py` (if your model is hosted locally).
2. Implement a new handler class for your model.
3. Update `bfcl/constants/model_config.py`.
4. Submit a Pull Request.

For detailed steps, please see the [Contributing Guide](./CONTRIBUTING.md).


## Additional Resources

- [Gorilla Discord](https://discord.gg/grXXvj9Whz) (`#leaderboard` channel)
- [Project Website](https://gorilla.cs.berkeley.edu/)

All the leaderboard statistics, and data used to train the models are released under Apache 2.0.
Gorilla is an open source effort from UC Berkeley and we welcome contributors.
Please email us your comments, criticisms, and questions. More information about the project can be found at [https://gorilla.cs.berkeley.edu/](https://gorilla.cs.berkeley.edu/)


## è£œè¶³ by Nejumi Leaderboard
### Nejumi Leaderboardã®ãŸã‚ã«è¡Œã£ãŸå¤‰æ›´
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€BFCLã‚’Nejumi Leaderboardã«çµ±åˆã™ã‚‹ãŸã‚ã«è¡Œã£ãŸå…·ä½“çš„ãªå¤‰æ›´ã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ã—ã¾ã™ã€‚

#### 1 è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ—¥æœ¬èªåŒ–
- qwen/qwen3-235b-a22bã‚’ç”¨ã„ã¦ç¿»è¨³
- llm-leaderboard/scripts/translation/bfcl_translation.pyã‚’åˆ©ç”¨
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯WandBã®artifactsã«ä¿å­˜ [link](https://wandb.ai/llm-leaderboard/nejumi-leaderboard4/artifacts/dataset/bfcl)
- **ãƒ«ãƒ¼ãƒ«**: é–¢æ•°åã€ã‚³ãƒ¼ãƒ‰é–¢é€£å†…å®¹ã¯ç¿»è¨³å¯¾è±¡å¤–

#### 2 çµ±åˆ
- `scripts/run_eval.py`ã«BFCLè©•ä¾¡ã‚’çµ±åˆ
- BFCLä¾å­˜é–¢ä¿‚ã«ä¼´ã†uv.lockã®æ›´æ–°ã¨uvãƒ™ãƒ¼ã‚¹ã®ä¾å­˜é–¢ä¿‚ç®¡ç†ã¸ã®ç§»è¡Œ
- `scripts/evaluator/bfcl.py`ã®ä½œæˆ
  - WandBConfigSingletonã¨ã®çµ±åˆ
  - è¨­å®šã®å‹•çš„ãƒãƒ¼ã‚¸ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ + ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šï¼‰
  - ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ï¼‰
  - WandB Artifactã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—
  - è©•ä¾¡çµæœã®WandBãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆ
- base_configã¸ã®è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¿½åŠ :
- bfclã‚’packageã¨ã—ã¦downloadã—ãªã„ã‚ˆã†ã«å¤‰æ›´ã€‚bfcl_pkgå†…ã®çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤‰æ›
- llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/bfcl/constants/eval_config.pyå†…ã®pathã‚’å¤‰æ›´
- llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/bfcl/eval_checker/multi_turn_eval/func_source_codeå†…ã®long_context.pyã‚’å®Ÿè¡Œæ™‚ã«pathã®å•é¡Œã§åˆ©ç”¨ã§ããªã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã£ãŸã®ã§ã€è©²å½“ãƒ•ã‚¡ã‚¤ãƒ«ã«long_context.pyå†…ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
- W&Bã¸ã®çµæœè¡¨ç¤º
  - W&Bã®Tableã«è©³ç´°ãªçµæœã‚’æ®‹ã™ãŸã‚ã«ã€å‡ºåŠ›ã•ã‚Œã‚‹score fileã«ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒè¿½åŠ ã•ã‚Œã‚‹ã‚ˆã†ã«å¤‰æ›´(æˆåŠŸãƒ»å¤±æ•—ä¸¡æ–¹ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§è©³ç´°æƒ…å ±ã‚’åŒ…å«)
- ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®config fileã«BFCLã®model idã‚’è¿½åŠ 

#### 3 llm-leadrboardã§èµ·å‹•ã•ã‚Œã‚‹vllmã‚’åˆ©ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´
- llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/bfcl/model_handler/local_inference/base_oss_handler.pyã®vllm_hostã¨portã‚’å¤‰æ›´

#### 4 ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®chat templateã¸ã®å¯¾å¿œ
- ã‚ªãƒªã‚¸ãƒŠãƒ«ã®BFCLã§ã¯ã€vllmèµ·å‹•æ™‚ã«chat templateã‚’åˆ©ç”¨ã›ãšã€æ¨è«–å®Ÿè¡Œæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®classã§templateã®å¯¾å¿œã‚’è¡Œãªã£ã¦ã„ãŸã€‚Nejumi leaderboardã§ã¯ã€vllmèµ·å‹•æ™‚ã«chat templateã‚’åˆ©ç”¨ã™ã‚‹ã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®classå†…ã§ã®chat templateã‚’å‰Šé™¤ã—ã€llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/bfcl/model_handler/local_inference/base_oss_handler.pyå†…ã§OSSHandlerå†…ã§Chat Completionå½¢å¼ã«å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®è¨­å®šé …ç›®ãŒå¤§å¹…ã«ç°¡ç´ åŒ–ã•ã‚Œã¾ã—ãŸã€‚
- ä¸è¦ã«ãªã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
  - **`_format_prompt`**: Chat Completions APIãŒå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’çµ±ä¸€ã™ã‚‹ãŸã‚ä¸è¦ã€‚ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®äºŒé‡é©ç”¨å•é¡Œã‚‚è§£æ±ºã•ã‚Œã‚‹
- ä¾ç„¶ã¨ã—ã¦å¿…è¦ãªãƒ¡ã‚½ãƒƒãƒ‰
  - **`decode_ast`/`decode_execute`**: å‡ºåŠ›ãƒ‘ãƒ¼ã‚¹ã¯æ¨¡å‹å›ºæœ‰ã®ãŸã‚å¿…è¦
  - **`_pre_query_processing_prompting`**: å‰å‡¦ç†ã¯æ¨¡å‹å›ºæœ‰ã®ãŸã‚å¿…è¦ã€‚è©³ç´°ã¯ä»¥ä¸‹ã§è§£èª¬ã—ã¾ã™ã€‚

### æ–°ã—ããƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã™ã‚‹æ–¹æ³•
- å…¬å¼ã®[Contributing Guide](./CONTRIBUTING.md)ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ä»¥ä¸‹ã€æ—¥æœ¬èªã§ã‚ã‹ã‚Šã‚„ã™ãè§£èª¬ & Nejumi Leaderboardã«ç‰¹åŒ–ã—ãŸå¯¾å¿œã«ã¤ã„ã¦è§£èª¬ã‚’ã—ã¾ã™ã€‚

#### OSSãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
1. `bfcl/model_handler/local_inference/base_oss_handler.py`ã‚’ç¢ºèªã—ã¤ã¤ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®æ–°ã—ã„handler classã‚’llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/bfcl/model_handler/local_inferenceã«ä½œæˆã—ã¦ãã ã•ã„ã€‚
  - handlerã®ä½œæˆã«ã¤ã„ã¦ã¯ã€ã“ã¡ã‚‰ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚
2. ãã®å¾Œ`bfcl/constants/model_config.py`ã«ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¿½åŠ ã—ã¾ã™ã€‚
3. modelã”ã¨ã®configå†…ã®bfcl_model_nameã«`bfcl/constants/model_config.py`ã«è¿½åŠ ã—ãŸãƒ¢ãƒ‡ãƒ«åã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„

#### APIã®å ´åˆ
1. `bfcl/model_handler/base_handler.py`ã‚’ç¢ºèªã—ã¤ã¤ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®æ–°ã—ã„handler classã‚’llm-leaderboard/scripts/evaluator/evaluate_utils/bfcl_pkg/bfcl/model_handler/api_inferenceã«ä½œæˆã—ã¦ä¸‹ã•ã„ã€‚
2. ãã®å¾Œ`bfcl/constants/model_config.py`ã«ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¿½åŠ ã—ã¾ã™ã€‚
3. modelã”ã¨ã®configå†…ã®bfcl_model_nameã«`bfcl/constants/model_config.py`ã«è¿½åŠ ã—ãŸãƒ¢ãƒ‡ãƒ«åã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„

## ä»•çµ„ã¿ç†è§£ã®ãŸã‚ã®è§£èª¬
### è³ªå•1: bfcl/model_handler/base_handler.py ã¯ä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ï¼Ÿ
**BaseHandlerã‚¯ãƒ©ã‚¹**ã¯ã€**BFCLï¼ˆBerkeley Function-calling Leaderboardï¼‰ã«ãŠã‘ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’è¡Œã†ãŸã‚ã®åŸºç›¤ã¨ãªã‚‹æŠ½è±¡ã‚¯ãƒ©ã‚¹**ã§ã™ã€‚

#### ğŸ¯ ä¸»è¦ãªå½¹å‰²ã¨æ©Ÿèƒ½

**1. ãƒ¢ãƒ‡ãƒ«æ¨è«–ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹**
- ç•°ãªã‚‹APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAIã€Claudeã€Geminiãªã©ï¼‰ã«å¯¾ã—ã¦å…±é€šã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›
- `inference()`ãƒ¡ã‚½ãƒƒãƒ‰ãŒæ¨è«–ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦æ©Ÿèƒ½
- Function Callingï¼ˆFCï¼‰ãƒ¢ãƒ¼ãƒ‰ã¨Promptingãƒ¢ãƒ¼ãƒ‰ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆ

**2. ã‚·ãƒ³ã‚°ãƒ«ã‚¿ãƒ¼ãƒ³ã¨ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®å¯¾è©±å‡¦ç†**
- `inference_single_turn_FC/prompting()`: å˜ç™ºã®è³ªå•å¿œç­”å‡¦ç†
- `inference_multi_turn_FC/prompting()`: è¤‡æ•°å›ã®å¯¾è©±ã‚’è¡Œã†å‡¦ç†
- ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã§ã¯é–¢æ•°ã®å®Ÿè¡Œçµæœã‚’æ¬¡ã®ã‚¿ãƒ¼ãƒ³ã«å¼•ãç¶™ãã€é€£ç¶šçš„ãªå¯¾è©±ãŒå¯èƒ½

**3. é–¢æ•°å‘¼ã³å‡ºã—ï¼ˆFunction Callingï¼‰ã®å®Ÿè¡Œç®¡ç†**
- ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰é–¢æ•°å®šç¾©ã‚’å–å¾—ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒé©åˆ‡ãªé–¢æ•°ã‚’å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ç®¡ç†
- é–¢æ•°ã®å®Ÿè¡Œçµæœã‚’å–å¾—ã—ã€æ¬¡ã®ã‚¯ã‚¨ãƒªã«åæ˜ 
- `MAXIMUM_STEP_LIMIT`ã«ã‚ˆã‚‹ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢æ©Ÿèƒ½

**4. ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®è¨ˆæ¸¬**
- å…¥åŠ›ãƒ»å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ­£ç¢ºãªè¨ˆæ¸¬
- APIå‘¼ã³å‡ºã—ã®å¿œç­”æ™‚é–“æ¸¬å®š
- è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦é‡è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®åé›†

**5. çŠ¶æ…‹ç®¡ç†ã¨ãƒ­ã‚°è¨˜éŒ²**
- ã‚¯ãƒ©ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®çŠ¶æ…‹å¤‰åŒ–ã‚’è¿½è·¡
- è©³ç´°ãªæ¨è«–ãƒ­ã‚°ã®è¨˜éŒ²ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
- å®Ÿè¡Œçµæœã®JSONå½¢å¼ã§ã®æ°¸ç¶šåŒ–

**6. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**
- ãƒ¢ãƒ‡ãƒ«å¿œç­”ã®ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—æ™‚ã®é©åˆ‡ãªå‡¦ç†
- ã‚¹ãƒ†ãƒƒãƒ—æ•°ä¸Šé™ã«ã‚ˆã‚‹å¼·åˆ¶çµ‚äº†æ©Ÿèƒ½
- å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã®æ•æ‰ã¨ãƒ­ã‚°è¨˜éŒ²

#### ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ
BaseHandlerã‚¯ãƒ©ã‚¹ã¯**ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³**ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€ä»¥ä¸‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦å®šç¾©ã•ã‚Œã€å„APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã®å…·ä½“çš„ãªå®Ÿè£…ãŒå¿…è¦ã§ã™ï¼š

**Function Callingãƒ¢ãƒ¼ãƒ‰ç”¨:**
- `_query_FC()`: APIã¸ã®å®Ÿéš›ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
- `_pre_query_processing_FC()`: ã‚¯ã‚¨ãƒªå‰ã®å‰å‡¦ç†
- `_compile_tools()`: é–¢æ•°å®šç¾©ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
- `_parse_query_response_FC()`: APIå¿œç­”ã®è§£æ
- `add_first_turn_message_FC()`: åˆå›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¿½åŠ 
- `_add_assistant_message_FC()`: ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã®è¿½åŠ 
- `_add_execution_results_FC()`: å®Ÿè¡Œçµæœã®è¿½åŠ 

**Promptingãƒ¢ãƒ¼ãƒ‰ç”¨:**
- `_query_prompting()`: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
- `_pre_query_processing_prompting()`: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‰å‡¦ç†
- `_parse_query_response_prompting()`: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¿œç­”ã®è§£æ
- å¯¾å¿œã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤

#### ğŸ’¡ FCãƒ¢ãƒ¼ãƒ‰ vs Promptingãƒ¢ãƒ¼ãƒ‰ã®é•ã„

| é …ç›® | FCãƒ¢ãƒ¼ãƒ‰ | Promptingãƒ¢ãƒ¼ãƒ‰ |
|------|----------|----------------|
| **å‡ºåŠ›å½¢å¼** | æ§‹é€ åŒ–ã•ã‚ŒãŸJSON | è‡ªç„¶è¨€èª+é–¢æ•°å‘¼ã³å‡ºã— |
| **ç²¾åº¦** | é«˜ã„ï¼ˆæ§‹é€ ãŒä¿è¨¼ï¼‰ | ä¸­ç¨‹åº¦ï¼ˆè§£æãŒå¿…è¦ï¼‰ |
| **å¯¾å¿œãƒ¢ãƒ‡ãƒ«** | OpenAIã€Claudeç­‰ã®æ–°ã—ã„ãƒ¢ãƒ‡ãƒ« | ã‚ˆã‚Šå¹…åºƒã„ãƒ¢ãƒ‡ãƒ« |
| **å®Ÿè£…ã®è¤‡é›‘ã•** | ã‚·ãƒ³ãƒ—ãƒ« | è¤‡é›‘ï¼ˆãƒ†ã‚­ã‚¹ãƒˆè§£æãŒå¿…è¦ï¼‰ |

**FCãƒ¢ãƒ¼ãƒ‰ã®ä¾‹:**
```python
# ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ï¼ˆæ§‹é€ åŒ–ï¼‰
{"tool_calls": [{"function": {"name": "get_weather", "arguments": "{\"location\": \"æ±äº¬\"}"}}]}
```

**Promptingãƒ¢ãƒ¼ãƒ‰ã®ä¾‹:**
```python
# ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ï¼ˆè‡ªç„¶è¨€èªï¼‰
"[get_weather(location='æ±äº¬')]"
# â†“ ASTè§£æãŒå¿…è¦
[{'get_weather': {'location': 'æ±äº¬'}}]
```

#### ğŸ”§ ASTè§£æï¼ˆAbstract Syntax Treeè§£æï¼‰ã®ä»•çµ„ã¿

Promptingãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸè‡ªç„¶è¨€èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰Pythonã®é–¢æ•°å‘¼ã³å‡ºã—ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã«ASTè§£æã‚’ä½¿ç”¨ã—ã¾ã™ï¼š

**1. ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†**
```python
# "[get_weather(location='æ±äº¬')]" â†’ "get_weather(location='æ±äº¬')"
cleaned_input = input_str.strip("[]'")
```

**2. Pythonã®ASTãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§æ§‹æ–‡è§£æ**
```python
parsed = ast.parse(cleaned_input, mode="eval")
```

**3. é–¢æ•°å‘¼ã³å‡ºã—ã¨å¼•æ•°ã®æŠ½å‡º**
```python
# æœ€çµ‚å‡ºåŠ›: [{'get_weather': {'location': 'æ±äº¬'}}]
```

#### âš¡ é–¢æ•°å®Ÿè¡Œã®ä»•çµ„ã¿

**é‡è¦**: APIãƒ¢ãƒ‡ãƒ«è‡ªä½“ã¯é–¢æ•°ã‚’å®Ÿè¡Œã—ã¾ã›ã‚“ã€‚å®Ÿéš›ã®é–¢æ•°å®Ÿè¡Œã¯BFCLã‚·ã‚¹ãƒ†ãƒ å´ã§è¡Œã‚ã‚Œã¾ã™ã€‚

**APIãƒ¢ãƒ‡ãƒ«ã®å½¹å‰²**: ã€Œä¿³å„ªã€
- é–¢æ•°å‘¼ã³å‡ºã—ã®æŒ‡ç¤ºã‚’ç”Ÿæˆã™ã‚‹ã®ã¿
- å®Ÿéš›ã®å‡¦ç†ã¯è¡Œã‚ãªã„

**BFCLã‚·ã‚¹ãƒ†ãƒ ã®å½¹å‰²**: ã€Œå®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³ã€
- å®Ÿéš›ã®Pythonã‚¯ãƒ©ã‚¹ã‚’å‹•çš„ã«ãƒ­ãƒ¼ãƒ‰
- é–¢æ•°ã‚’å®Ÿéš›ã«å®Ÿè¡Œï¼ˆ`eval()`ä½¿ç”¨ï¼‰
- å®Ÿè¡Œçµæœã‚’ãƒ¢ãƒ‡ãƒ«ã«è¿”å´

```python
# å®Ÿéš›ã®é–¢æ•°å®Ÿè¡Œãƒ—ãƒ­ã‚»ã‚¹
def execute_multi_turn_func_call():
    # 1. å®Ÿéš›ã®Pythonã‚¯ãƒ©ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
    class_instance = TradingBot()
    
    # 2. é–¢æ•°å®Ÿè¡Œ
    result = eval("class_instance.place_order(symbol='AAPL', amount=100)")
    
    # 3. çµæœã‚’ãƒ¢ãƒ‡ãƒ«ã«è¿”å´
    return result
```

### è³ªå•2: bfcl/model_handler/api_inferenceã§å„ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ï¼Ÿ

api_inferenceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯**20å€‹ä»¥ä¸Šã®APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å°‚ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼**ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€ãã‚Œãã‚ŒãŒBaseHandlerã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã¦ç‰¹å®šã®APIä»•æ§˜ã«å¯¾å¿œã—ãŸå®Ÿè£…ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

#### ğŸ”§ å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å…±é€šå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

**å„ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¯ä»¥ä¸‹ã‚’å¿…ãšå®Ÿè£…:**
1. **APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–**: å„ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã®èªè¨¼ã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
2. **ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š**: `ModelStyle`enumå€¤ã®è¨­å®š
3. **ã‚¯ã‚¨ãƒªãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…**: `_query_FC()`ã¨`_query_prompting()`
4. **å¿œç­”è§£æã®å®Ÿè£…**: APIå›ºæœ‰ã®å¿œç­”å½¢å¼ã‹ã‚‰ã®æ¨™æº–å½¢å¼ã¸ã®å¤‰æ›
5. **ãƒ‡ã‚³ãƒ¼ãƒ‰æ©Ÿèƒ½**: `decode_ast()`ã¨`decode_execute()`ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
6. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: APIå›ºæœ‰ã®ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ç­‰ï¼‰ã¸ã®å¯¾å¿œ

#### ğŸ¢ ä¸»è¦APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç‰¹å¾´çš„ãªé•ã„

**1. openai.py - OpenAIHandler**
```python
class OpenAIHandler(BaseHandler):
    def __init__(self, model_name, temperature):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    def _query_FC(self, inference_data: dict):
        # ã‚·ãƒ³ãƒ—ãƒ«ã§æ¨™æº–çš„
        return self.generate_with_backoff(
            messages=messages,
            model="gpt-4",
            tools=tools,
            temperature=0.7  # ãŸã ã—o1ãƒ¢ãƒ‡ãƒ«ã§ã¯ä½¿ç”¨ä¸å¯
        )
```
**ç‰¹å¾´:**
- âœ… æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…
- âœ… æ¨™æº–çš„ãªFunction Callingå½¢å¼
- âš ï¸ o1/o3-miniãƒ¢ãƒ‡ãƒ«ã¯æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿éå¯¾å¿œ

**2. claude.py - ClaudeHandler**
```python
class ClaudeHandler(BaseHandler):
    def _query_FC(self, inference_data: dict):
        # ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ©Ÿèƒ½ä»˜ã
        if inference_data["caching_enabled"]:
            # ç›´è¿‘2ã¤ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            for message in reversed(messages):
                if message["role"] == "user":
                    message["content"][0]["cache_control"] = {"type": "ephemeral"}
        
        return self.generate_with_backoff(
            model="claude-3-sonnet",
            messages=messages_with_cache_control,
            tools=tools,
            max_tokens=8192  # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç•°ãªã‚‹
        )
```
**ç‰¹å¾´:**
- ğŸš€ **ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ©Ÿèƒ½**: ç›´è¿‘2ã¤ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
- ğŸ“ **å¯å¤‰ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™**: Opusã¯4096ã€Sonnetã¯8192
- ğŸ”„ **ç‰¹æ®Šãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†**: cache_control ãƒ•ãƒ©ã‚°ã‚’å‹•çš„ã«ç®¡ç†

**3. gemini.py - GeminiHandler**
```python
class GeminiHandler(BaseHandler):
    def _query_FC(self, inference_data: dict):
        # Google Cloudç‰¹æœ‰ã®è¤‡é›‘ãªå¤‰æ›
        func_declarations = []
        for function in inference_data["tools"]:
            func_declarations.append(
                FunctionDeclaration(
                    name=function["name"],
                    description=function["description"],
                    parameters=function["parameters"],
                )
            )
        
        tools = [Tool(function_declarations=func_declarations)]
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚‹å ´åˆã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå†ä½œæˆ
        if "system_prompt" in inference_data:
            client = GenerativeModel(
                self.model_name,
                system_instruction=inference_data["system_prompt"]
            )
```
**ç‰¹å¾´:**
- ğŸ”§ **è¤‡é›‘ãªå¤‰æ›å‡¦ç†**: é–¢æ•°ã‚’FunctionDeclarationâ†’Toolã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
- ğŸ—ï¸ **å‹•çš„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç”Ÿæˆ**: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚‹å ´åˆã¯ãƒ¢ãƒ‡ãƒ«å†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
- ğŸŒ **Google Cloudçµ±åˆ**: Vertex AIçµŒç”±ã§ã®ã‚¢ã‚¯ã‚»ã‚¹

**4. ãã®ä»–ã®å°‚ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼**
- **mistral.py**: Mistral AI APIå¯¾å¿œã€ç‹¬è‡ªã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å½¢å¼
- **cohere.py**: Cohere APIå¯¾å¿œã€ç‹¬è‡ªã®ãƒ„ãƒ¼ãƒ«å®šç¾©å½¢å¼
- **yi.py**: Yi AI APIå¯¾å¿œ
- **deepseek.py**: DeepSeek APIå¯¾å¿œ
- **databricks.py**: Databricks APIå¯¾å¿œ
- **nova.py**: Nova APIå¯¾å¿œ
- **nexus.py**: Nexus APIå¯¾å¿œï¼ˆã‚»ãƒŸã‚³ãƒ­ãƒ³åŒºåˆ‡ã‚Šå½¢å¼ï¼‰
- **gorilla.py**: Gorilla APIå¯¾å¿œ
- **fireworks.py**: Fireworks AI APIå¯¾å¿œ
- **nvidia.py**: NVIDIA APIå¯¾å¿œ
- **writer.py**: Writer APIå¯¾å¿œ
- **novita.py**: Novita APIå¯¾å¿œ
- **qwq.py**: QwQ APIå¯¾å¿œ
- **grok.py**: xAI Grok APIå¯¾å¿œ

#### ğŸ“Š å®Ÿè£…ã®è¤‡é›‘ã•æ¯”è¼ƒ

| API | å®Ÿè£…è¤‡é›‘åº¦ | ç‰¹æ®Šæ©Ÿèƒ½ | æ³¨æ„ç‚¹ |
|-----|-------------|----------|--------|
| **OpenAI** | â­â­ | o1ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ | æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ« |
| **Claude** | â­â­â­ | ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚° | ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ãŒç‰¹æ®Š |
| **Gemini** | â­â­â­â­ | å‹•çš„ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆ | Google Cloudè¨­å®šå¿…è¦ |
| **Cohere** | â­â­â­ | ç‹¬è‡ªãƒ„ãƒ¼ãƒ«å½¢å¼ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒå¤‰æ› |
| **ãã®ä»–** | â­â­ | åŸºæœ¬çš„ãªå®Ÿè£… | OpenAIäº’æ›ãŒå¤šã„ |

#### ğŸ¨ Promptingãƒ¢ãƒ¼ãƒ‰ã§ã®ç‰¹æ®Šå‡¦ç†ä¾‹

**Hermesï¼ˆXMLã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ï¼‰**
```python
def decode_ast(self, result):
    lines = result.split("\n")
    func_call = []
    for line in lines:
        if "<tool_call>" == line:
            flag = True
        elif "</tool_call>" == line:
            flag = False
        elif flag:
            tool_result = json.loads(line)
            func_call.append({tool_result["name"]: tool_result["arguments"]})
    return func_call
```

**MiningHandlerï¼ˆç‰¹æ®Šãƒ‘ãƒ¼ã‚¹ï¼‰**
```python
def _parse_query_response_prompting(self, api_response):
    # <tool_calls>ã‚¿ã‚°å†…ã®JSONã‚’æŠ½å‡º
    match = re.search(r'<tool_calls>\n(.*?)\n</tool_calls>', content, re.DOTALL)
    if match:
        tool_calls = match.group(1).strip()
        tool_calls = json.loads(tool_calls.replace("'",'"'))
    return {"model_responses": tool_calls, ...}
```


### è³ªå•3: bfcl/model_handler/local_inference/base_oss_handler.pyãŒã‚„ã£ã¦ã„ã‚‹ã“ã¨ã‚’æ•™ãˆã¦

**base_oss_handler.py**ã¯ã€**OSSï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ï¼‰ãƒ¢ãƒ‡ãƒ«ã€ã¤ã¾ã‚Šãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ç”¨ã®åŸºç›¤ã‚¯ãƒ©ã‚¹**ã§ã™ã€‚BaseHandlerã‚’ç¶™æ‰¿ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ç‰¹æœ‰ã®å‡¦ç†ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

#### ğŸ—ï¸ ä¸»è¦ãªå½¹å‰²ã¨æ©Ÿèƒ½

##### **1. Chat Completions API ã¸ã®å¯¾å¿œï¼ˆé‡è¦ãªå¤‰æ›´ç‚¹ï¼‰**
**å¾“æ¥ã®BFCL**: å„ãƒ¢ãƒ‡ãƒ«ã§å€‹åˆ¥ã«chat templateã‚’å‡¦ç†
```python
# æ—§å®Ÿè£…ï¼ˆå‰Šé™¤æ¸ˆã¿ï¼‰
def _format_prompt(self, messages, function):
    # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«å€‹åˆ¥ã®chat templateå‡¦ç†
    formatted_prompt = apply_chat_template(messages)
    return formatted_prompt
```

**ç¾åœ¨ã®Nejumi leaderboard**: vLLMã‚µãƒ¼ãƒãƒ¼å´ã§chat templateã‚’çµ±ä¸€å‡¦ç†
```python
# æ–°å®Ÿè£…
def _query_prompting(self, inference_data: dict):
    # Chat Completions APIã§ã¯vLLMã‚µãƒ¼ãƒãƒ¼å´ã§chat templateãŒé©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€
    # _format_promptã¯ä½¿ç”¨ã›ãšã€ç›´æ¥messagesã‚’é€ä¿¡ã™ã‚‹
    api_response = self.client.chat.completions.create(
        model=self.model_path_or_id,
        temperature=self.temperature,
        messages=message,  # ç›´æ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
        max_tokens=leftover_tokens_count,
    )
```

##### **2. vLLMã‚µãƒ¼ãƒãƒ¼ã¨ã®é€šä¿¡ç®¡ç†**
```python
class OSSHandler(BaseHandler):
    def __init__(self, model_name, temperature, dtype="bfloat16"):
        # vLLMã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šè¨­å®š
        self.vllm_host = os.getenv("VLLM_ENDPOINT", "localhost")
        self.vllm_port = os.getenv("VLLM_PORT", VLLM_PORT)
        self.base_url = f"http://{self.vllm_host}:{self.vllm_port}/v1"
        self.client = OpenAI(base_url=self.base_url, api_key="EMPTY")
```

##### **3. ãƒãƒƒãƒæ¨è«–ã®å®Ÿè£…**
APIãƒ¢ãƒ‡ãƒ«ã¨ç•°ãªã‚Šã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã¯**ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ã‹ã‚‰ãƒãƒƒãƒã§å‡¦ç†**ã™ã‚‹ã“ã¨ã§åŠ¹ç‡åŒ–ï¼š

```python
def batch_inference(self, test_entries, num_gpus, gpu_memory_utilization, ...):
    # 1. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
    self.tokenizer = AutoTokenizer.from_pretrained(**load_kwargs)
    config = AutoConfig.from_pretrained(**load_kwargs)
    
    # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã®è¨­å®š
    if hasattr(config, "max_position_embeddings"):
        self.max_context_length = config.max_position_embeddings
    
    # 3. ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œ
    # (å€‹åˆ¥ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ä¸€åº¦ã«ã¾ã¨ã‚ã¦å‡¦ç†)
```

##### **4. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ã‚³ãƒ¼ãƒ‰å‡¦ç†**
```python
@override
def decode_ast(self, result, language="Python"):
    return default_decode_ast_prompting(result, language)

@override
def decode_execute(self, result):
    return default_decode_execute_prompting(result)
```

##### **5. ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¨å®š**
```python
# Chat Completions APIã§ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
messages_text = " ".join([msg.get("content", "") for msg in message])
input_token_count = len(self.tokenizer.tokenize(messages_text))
```

#### âš¡ å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
1. ãƒãƒƒãƒæ¨è«–é–‹å§‹
   â†“
2. ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ (vLLMã‚µãƒ¼ãƒãƒ¼ãŒã™ã§ã«èµ·å‹•ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—)
   â†“
3. vLLMã‚µãƒ¼ãƒãƒ¼ã¨ã®æ¥ç¶šç¢ºç«‹
   â†“
4. ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®å‰å‡¦ç†
   â†“
5. Chat Completions APIçµŒç”±ã§ã‚¯ã‚¨ãƒª
   â†“
6. å¿œç­”ã®è§£æãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰
   â†“
7. çµæœã®ä¿å­˜
```

### è³ªå•4: bfcl/model_handler/local_inferenceå†…ã®è¿½åŠ ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ©ã‚¹ãŒä½•ã‚’ã—ã¦ã„ã‚‹ã‹ã‚’æ•™ãˆã¦


local_inferenceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯**25å€‹ä»¥ä¸Šã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«å°‚ç”¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼**ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€base_oss_handler.pyã®**OSSHandler**ã‚’ç¶™æ‰¿ã—ã¦ã€å„ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®å‡¦ç†ã‚’æœ€å°é™ã®å®Ÿè£…ã§æä¾›ã—ã¦ã„ã¾ã™ã€‚

#### **Nejumi Leaderboardã®ãŸã‚ã«å‰Šé™¤ã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰**
- **`_format_prompt`**: Chat Completions APIãŒvLLMã‚µãƒ¼ãƒãƒ¼å´ã§çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å‡¦ç†ã™ã‚‹ãŸã‚ä¸è¦

#### **ä¾ç„¶ã¨ã—ã¦å¿…è¦ãªãƒ¡ã‚½ãƒƒãƒ‰**
- **`decode_ast`/`decode_execute`**: å‡ºåŠ›ãƒ‘ãƒ¼ã‚¹ã¯ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãŸã‚å¿…è¦
- **`_pre_query_processing_prompting`**: å‰å‡¦ç†ã¯ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãŸã‚å¿…è¦
- **`_add_execution_results_prompting`**: å®Ÿè¡Œçµæœã®å‡¦ç†æ–¹æ³•ãŒãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç•°ãªã‚‹

#### ğŸ¨ ãƒ¢ãƒ‡ãƒ«åˆ¥ã®å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨å¯¾å¿œãŒå¿…è¦ãªç†ç”±ã¨å…·ä½“ä¾‹

#### **1. ã‚·ãƒ³ãƒ—ãƒ«ãªã‚±ãƒ¼ã‚¹: hammer.py**
```python
class HammerHandler(OSSHandler):
    @override
    def decode_ast(self, result, language="Python"):
        # å˜ç´”ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— + ç›´æ¥JSONãƒ‘ãƒ¼ã‚¹
        result = result.replace("```", "")
        try:
            result = json.loads(result)
        except:
            result = []
        
        decoded_output = []
        for invoked_function in result:
            name = invoked_function["name"]
            params = invoked_function["arguments"]
            decoded_output.append({name: params})
        return decoded_output
```

**æœŸå¾…ã•ã‚Œã‚‹æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:**
```json
[{"name": "function_name", "arguments": {"param": "value"}}]
```

#### **2. ç‰¹æ®Šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ: deepseek.py**
```python
class DeepseekHandler(OSSHandler):
    @override
    def decode_ast(self, result, language="Python"):
        result = result.strip()
        # ```json ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
        if result.startswith("```json"):
            result = result[len("```json"):]
        if result.startswith("```python"):
            result = result[len("```python"):]
        return super().decode_ast(result, language)
```

**DeepSeekã®å®Ÿéš›ã®å‡ºåŠ›ä¾‹:**
```
```json
{"name": "calculate", "arguments": {"x": 5, "y": 10}}
```
```

#### **3. è¤‡é›‘ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: llama_3_1.py**
```python
class Llama31Handler(OSSHandler):
    @override
    def decode_ast(self, result, language="Python"):
        # ã‚¿ã‚°é™¤å»ã€ã‚»ãƒŸã‚³ãƒ­ãƒ³åŒºåˆ‡ã‚Šå¯¾å¿œ
        result = result.replace("<|python_tag|>", "").strip()
        calls = result.split(";")
        return [json.loads(call.strip()) for call in calls if call.strip()]
```

**Llama 3.1ã®å®Ÿéš›ã®å‡ºåŠ›ä¾‹:**
```
<|python_tag|>{"name": "calc", "arguments": {...}}; {"name": "func2", "arguments": {...}}
```

#### **4. è¶…è¤‡é›‘ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: minicpm_fc.py**
```python
def fc2dict(sequence: str, 
           tool_call_start="<|tool_call_start|>",
           tool_call_end="<|tool_call_end|>",
           thought_start="<|thought_start|>",
           thought_end="<|thought_end|>"):
    # æ€è€ƒéç¨‹ã¨ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ã‚¿ã‚°ã‚’å«ã‚€è¤‡é›‘ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    if thought_end in sequence and thought_start in sequence:
        thought_string, sequence = sequence.rsplit(thought_end, 1)
        thought_string = thought_string.split(thought_start, 1)[1]
    
    if tool_call_start in sequence and tool_call_end in sequence:
        tool_call_string, content = sequence.rsplit(tool_call_end, 1)
        tool_call_string = tool_call_string.split(tool_call_start, 1)[1]
        # ASTè§£æã§é–¢æ•°å‘¼ã³å‡ºã—ã‚’æŠ½å‡º
        parsed = ast.parse(tool_call_string)
        # ...
```

**MiniCPMã®å®Ÿéš›ã®å‡ºåŠ›ä¾‹:**
```
<|thought_start|>
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è¨ˆç®—ã‚’æ±‚ã‚ã¦ã„ã‚‹ã®ã§ã€calculateé–¢æ•°ã‚’ä½¿ã„ã¾ã™
<|thought_end|>
<|tool_call_start|>
```python
calculate(x=5, y=10)
```
<|tool_call_end|>
è¨ˆç®—çµæœã‚’ãŠè¦‹ã›ã—ã¾ã™
```

### ğŸ”„ å®Ÿè¡Œçµæœã®å‡¦ç†æ–¹æ³•ã®é•ã„

#### **æ¨™æº–çš„ãªå‡¦ç†ï¼ˆDeepSeekï¼‰**
```python
def _add_execution_results_prompting(self, inference_data, execution_results, model_response_data):
    # DeepSeekã¯toolãƒ­ãƒ¼ãƒ«ã‚’å—ã‘ä»˜ã‘ãªã„ãŸã‚ã€userãƒ­ãƒ¼ãƒ«ã‚’ä½¿ç”¨
    tool_message = {"role": "user", "content": []}
    for execution_result, decoded_model_response in zip(execution_results, model_response_data["model_responses_decoded"]):
        tool_message["content"].append({
            "role": "tool",
            "name": decoded_model_response,
            "content": execution_result,
        })
    inference_data["message"].append(tool_message)
```

#### **ç‰¹æ®Šãªãƒ­ãƒ¼ãƒ«ä½¿ç”¨ï¼ˆLlamaï¼‰**
```python
def _add_execution_results_prompting(self, inference_data, execution_results, model_response_data):
    for execution_result in execution_results:
        # Llamaã¯ç‰¹æ®Šãª`ipython`ãƒ­ãƒ¼ãƒ«ã‚’ä½¿ç”¨
        inference_data["message"].append({
            "role": "ipython",
            "content": execution_result,
        })
```

### ğŸ“Š ãƒ¢ãƒ‡ãƒ«åˆ¥ç‰¹å¾´ã¾ã¨ã‚

| ãƒ¢ãƒ‡ãƒ« | å‡ºåŠ›ã®ç‰¹å¾´ | ä¸»ãªå‡¦ç† |
|--------|------------|----------|
| **Hammer** | æ¨™æº–JSON | æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ« |
| **DeepSeek** | ```json\n...\n``` | ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹é™¤å» |
| **Llama 3.1** | <python_tag>...;... | ã‚¿ã‚°é™¤å»+ã‚»ãƒŸã‚³ãƒ­ãƒ³åˆ†å‰² |
| **MiniCPM** | æ€è€ƒéç¨‹+ãƒ„ãƒ¼ãƒ«ã‚¿ã‚° | è¤‡é›‘ãªã‚¿ã‚°è§£æ |
| **Phi** | ```json/python... | è¤‡æ•°ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹å¯¾å¿œ |
| **GLM** | æ”¹è¡ŒåŒºåˆ‡ã‚Š | ç‰¹æ®Šãªæ”¹è¡Œå‡¦ç† |
| **Granite** | <function_call>... | XMLãƒ©ã‚¤ã‚¯ã‚¿ã‚° |

#### ğŸ’¡ å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒç•°ãªã‚‹ç†ç”±

**1. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®é•ã„**
- å„ãƒ¢ãƒ‡ãƒ«ãŒç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã•ã‚Œã¦ã„ã‚‹ãŸã‚

**2. ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é•ã„**
- ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¦å‰‡ãŒã‚ã‚‹ãŸã‚

**3. è¨­è¨ˆæ€æƒ³ã®é•ã„**
- å‡ºåŠ›ã®è©³ç´°ã•ã‚„æ§‹é€ ã«å¯¾ã™ã‚‹è€ƒãˆæ–¹ãŒç•°ãªã‚‹ãŸã‚


