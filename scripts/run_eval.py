import os
if os.environ.get("NEJUMI_MAIN_STARTED") == "1":
    print("Duplicate invocation detected; skipping.")
    raise SystemExit(0)
os.environ["NEJUMI_MAIN_STARTED"] = "1"

import wandb
from pathlib import Path
from argparse import ArgumentParser
from omegaconf import OmegaConf
import questionary

from config_singleton import WandbConfigSingleton
from llm_inference_adapter import get_llm_inference_engine
from vllm_server import shutdown_vllm_server
from docker_vllm_manager import stop_vllm_container_if_needed, start_vllm_container_if_needed
from blend_run import blend_run
from evaluator import (
    jaster,
    jbbq,
    mtbench,
    jaster_translation,
    toxicity,
    bfcl,
    jtruthfulqa,
    hle,
    hallulens,
    m_ifeval,
    aggregate,
    swe_bench,
    arc_agi,
)
from utils import paginate_choices
import weave

# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from evaluator.evaluate_utils.progress_tracker import (
    initialize_progress_tracker, start_benchmark_tracking, 
    complete_benchmark_tracking, finish_progress_tracking
)
from evaluator.evaluate_utils.validation_helpers import validate_all_benchmarks

# Set config path
config_dir = Path("configs")
base_cfg_name = "base_config.yaml"
parser = ArgumentParser()
parser.add_argument("--config", "-c", type=str)
parser.add_argument("--select-config", "-s", action="store_true", default=False)
args = parser.parse_args()

if args.select_config:
    choices = sorted([p.name for p in config_dir.iterdir() if p.suffix == ".yaml"])
    if len(choices) > 36:
        custom_cfg_name = paginate_choices(choices)
    else:
        custom_cfg_name = questionary.select(
            "Select config",
            choices=choices,
            use_shortcuts=True,
        ).ask()
    custom_cfg_path = config_dir / custom_cfg_name
elif args.config:
    custom_cfg_path = config_dir / args.config
else:
    raise ValueError("No arguments found. Please specify either --config or --select-config.")

if custom_cfg_path.suffix != ".yaml":
    custom_cfg_path = custom_cfg_path.with_suffix(".yaml")
assert custom_cfg_path.exists(), f"Config file {custom_cfg_path.resolve()} does not exist"

# Configuration loading
custom_cfg = OmegaConf.load(custom_cfg_path)
base_cfg_path = config_dir / base_cfg_name
base_cfg = OmegaConf.load(base_cfg_path)

# vLLMåˆ©ç”¨æ™‚ã«base_urlãŒæœªæŒ‡å®šã®å ´åˆã€Composeã‚µãƒ¼ãƒ“ã‚¹å 'vllm' ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
if "api" in custom_cfg and custom_cfg.api in ["vllm", "vllm-docker"]:
    if "base_url" not in custom_cfg:
        print("INFO: api is vllm/vllm-docker and base_url is not set. Defaulting to http://vllm:8000/v1")
        custom_cfg.base_url = "http://vllm:8000/v1"

custom_cfg = OmegaConf.merge(base_cfg, custom_cfg)
cfg_dict = OmegaConf.to_container(custom_cfg, resolve=True)
assert isinstance(cfg_dict, dict), "instance.config must be a DictConfig"

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
def get_api_key_from_env(service_name):
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—"""
    env_var_name = f"{service_name.upper()}_API_KEY"
    return os.getenv(env_var_name)

# W&B APIã‚­ãƒ¼ã®è¨­å®š
wandb_api_key = get_api_key_from_env("wandb")
if wandb_api_key:
    os.environ["WANDB_API_KEY"] = wandb_api_key
    print(f"Wandb API key loaded from environment variable")
else:
    print("Warning: WANDB_API_KEY not found in environment variables")

"""Safeguard W&B init in multi-run environments"""
wandb_run = os.environ.get("WANDB_RUN_ID")
try:
    if os.environ.get("NEJUMI_WANDB_INIT_DONE") == "1":
        print("W&B already initialized; reusing existing run context.")
        run = wandb.run  # may be None if not active
    else:
        wandb.login()
        run = wandb.init(
            entity=cfg_dict["wandb"]["entity"],
            project=cfg_dict["wandb"]["project"],
            name=cfg_dict["wandb"]["run_name"],
            config=cfg_dict,
            job_type="evaluation",
            resume="allow" if wandb_run else None,
        )
        os.environ["NEJUMI_WANDB_INIT_DONE"] = "1"
        weave.init(cfg_dict["wandb"]["entity"]+"/"+cfg_dict["wandb"]["project"])
except Exception as e:
    print(f"Warning: Failed to initialize Wandb: {e}")
    print("Continuing without Wandb logging...")
    run = None

# Initialize the WandbConfigSingleton
if run:
    WandbConfigSingleton.initialize(run, llm=None)
    cfg = WandbConfigSingleton.get_instance().config

    # Save configuration as artifact
    artifact = wandb.Artifact("config", type="config")
    artifact.add_file(custom_cfg_path)
    run.log_artifact(artifact)

    # Inherit old runs
    blend_run(run_chain=True)
else:
    # WandbãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ä»£æ›¿è¨­å®š
    cfg = OmegaConf.create(cfg_dict)

# æœ‰åŠ¹ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒªã‚¹ãƒˆã‚’å–å¾—
enabled_benchmarks = []
benchmark_map = {
    'bfcl': 'bfcl',
    'swebench': 'swebench', 
    'mtbench': 'mtbench',
    'jbbq': 'jbbq',
    'toxicity': 'toxicity',
    'jtruthfulqa': 'jtruthfulqa',
    'hle': 'hle',
    'hallulens': 'hallulens',
    'arc_agi': 'arc_agi',
    'm_ifeval': 'm_ifeval',
    'jaster': 'jaster'
}

for bench_key, bench_name in benchmark_map.items():
    if getattr(cfg.run, bench_key, False):
        enabled_benchmarks.append(bench_name)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
print("\n" + "="*80)
print("ğŸ” GLOBAL TOKEN ALLOCATION VALIDATION")
print("="*80)
try:
    validation_results = validate_all_benchmarks(cfg)
    
    has_warnings = False
    has_errors = False
    
    for benchmark, (is_valid, message) in validation_results.items():
        if benchmark in enabled_benchmarks:  # æœ‰åŠ¹ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ã¿ãƒã‚§ãƒƒã‚¯
            if not is_valid:
                if "âŒ" in message:
                    has_errors = True
                else:
                    has_warnings = True
            print(message)
    
    print("="*80)
    
    if has_errors:
        print("\nâŒ CRITICAL: Some benchmarks have insufficient output tokens!")
        print("   This will likely cause empty responses and unfairly low scores.")
        response = input("\nContinue anyway? (y/N): ").strip().lower()
        if response not in ['y', 'yes']:
            print("Evaluation aborted by user.")
            if run:
                run.finish()
            exit(1)
    elif has_warnings:
        print("\nâš ï¸  WARNING: Some benchmarks have suboptimal token allocation.")
        response = input("\nContinue? (Y/n): ").strip().lower()
        if response in ['n', 'no']:
            print("Evaluation aborted by user.")
            if run:
                run.finish()
            exit(1)
    else:
        print("\nâœ… All token allocations look good!")
        
except Exception as e:
    print(f"âš ï¸  Token validation failed: {e}")
    print("Proceeding with evaluation...")

# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’åˆæœŸåŒ–
tracker = initialize_progress_tracker(enabled_benchmarks)
tracker.start_tracking()

# vLLMã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•å‡¦ç†ã‚’è¿½åŠ 
# Start vLLM container if needed (for vllm/vllm-docker API types)
# # Start vLLM container if needed
# if cfg.api in ["vllm-docker"]:
#     print(f"Starting vLLM container for model: {cfg.model.pretrained_model_name_or_path}")
#     
#     # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šï¼ˆã‚³ãƒ³ãƒ†ãƒŠãŒæ­£ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ï¼‰
#     os.environ["EVAL_CONFIG_PATH"] = str(custom_cfg_path.name)
#     
#     # vLLMã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•ï¼ˆãƒ¢ãƒ‡ãƒ«åã‚’æ˜ç¤ºçš„ã«æ¸¡ã™ï¼‰
#     success = start_vllm_container_if_needed(model_name=cfg.model.pretrained_model_name_or_path)
#     if success is False:
#         print("Failed to start vLLM container, aborting evaluation")
#         if run:
#             wandb.finish()
#         exit(1)
#     elif success is True:
#         print("vLLM container started successfully")

# Start inference server
llm = get_llm_inference_engine()
if run:
    instance = WandbConfigSingleton.get_instance()
    instance.llm = llm

# BFCL
if cfg.run.bfcl:
    start_benchmark_tracking('bfcl')
    bfcl.evaluate()
    complete_benchmark_tracking('bfcl')

# SWE-Bench Verified evaluation
if cfg.run.swebench:
    start_benchmark_tracking('swebench')
    if cfg.swebench.background_eval:
        # è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹ã®å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ãŸã‚ã€ä»–ã®ãƒ™ãƒ³ãƒã¨ä¸¦è¡Œã§ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œã™ã‚‹
        # evaluate() ã¯ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆwait_and_log_metricsï¼‰ã‚’è¿”ã™å®Ÿè£…ã«çµ±ä¸€
        swebench_postprocess = swe_bench.evaluate()
    else:
        swe_bench.evaluate()
    complete_benchmark_tracking('swebench')

# mt-bench evaluation
if cfg.run.mtbench:
    start_benchmark_tracking('mtbench')
    mtbench.evaluate()
    complete_benchmark_tracking('mtbench')

# jbbq
if cfg.run.jbbq:
    start_benchmark_tracking('jbbq')
    jbbq.evaluate()
    complete_benchmark_tracking('jbbq')

# toxicity
if cfg.run.toxicity:
    start_benchmark_tracking('toxicity')
    toxicity.evaluate()
    complete_benchmark_tracking('toxicity')

# JTruthfulQA
if cfg.run.jtruthfulqa:
    start_benchmark_tracking('jtruthfulqa')
    jtruthfulqa.evaluate()
    complete_benchmark_tracking('jtruthfulqa')

# hle
if cfg.run.hle:
    start_benchmark_tracking('hle')
    hle.evaluate()
    complete_benchmark_tracking('hle')

# HalluLens
if cfg.run.hallulens:
    start_benchmark_tracking('hallulens')
    hallulens.evaluate()
    complete_benchmark_tracking('hallulens')

# ARC-AGI
if cfg.run.arc_agi:
    start_benchmark_tracking('arc_agi')
    arc_agi.evaluate()
    complete_benchmark_tracking('arc_agi')

# M-IFEval
if cfg.run.m_ifeval:
    start_benchmark_tracking('m_ifeval')
    m_ifeval.evaluate()
    complete_benchmark_tracking('m_ifeval')

# Evaluation phase
if cfg.run.jaster:
    start_benchmark_tracking('jaster')
    # llm-jp-eval evaluation
    jaster.evaluate()
    complete_benchmark_tracking('jaster')

    #### open weight model base evaluation
    # 1. evaluation for translation task in jaster with comet
    # APIã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦vLLMã‚µãƒ¼ãƒãƒ¼/ã‚³ãƒ³ãƒ†ãƒŠã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
    lifecycle_mode = cfg.vllm.get("lifecycle", "auto")
    
    # jaster_translation (COMET) ã‚„ jtruthfulqa (RoBERTa) ã¯GPUãƒ¡ãƒ¢ãƒªã‚’å¤§ããæ¶ˆè²»ã™ã‚‹ãŸã‚ã€
    # vLLMã‚’ä¸€æ™‚åœæ­¢ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    # lifecycle: 'always_on' ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã‚’é™¤ã
    if lifecycle_mode != 'always_on':
        if cfg.api == "vllm-local":
            shutdown_vllm_server()
        elif cfg.api in ["vllm", "vllm-docker"]:
            stop_vllm_container_if_needed()
    
    # COMETè©•ä¾¡ã‚’å®Ÿè¡Œ
    jaster_translation.evaluate()
    
    # APIã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦vLLMã‚µãƒ¼ãƒãƒ¼/ã‚³ãƒ³ãƒ†ãƒŠã‚’å†èµ·å‹•
    if lifecycle_mode != 'always_on':
        if cfg.api == "vllm-local":
            llm = get_llm_inference_engine()
            if run:
                instance.llm = llm
        elif cfg.api in ["vllm", "vllm-docker"]:
            start_vllm_container_if_needed(model_name=cfg.model.pretrained_model_name_or_path)
            # llmã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯åŒã˜ã‚‚ã®ã‚’ä½¿ã„ç¶šã‘ã‚‹
            pass

if cfg.run.swebench and cfg.swebench.background_eval:
    # SWE-Benchè©•ä¾¡å®Œäº†ã‚’å¾…ã£ã¦ã‹ã‚‰é›†è¨ˆãƒ»W&Bãƒ­ã‚®ãƒ³ã‚°ã‚’ç¢ºå®Ÿã«å®Ÿæ–½
    if callable(swebench_postprocess):
        swebench_postprocess()
    else:
        print("SWE-Bench background eval returned no callback; skipping explicit wait.")

# Aggregation
if cfg.run.aggregate:
    aggregate.evaluate()

# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ‚äº†
finish_progress_tracking()

# è©•ä¾¡å®Œäº†å¾Œã€vLLMã‚³ãƒ³ãƒ†ãƒŠã‚’åœæ­¢
if cfg.api in ["vllm", "vllm-docker"]:
    print("Stopping vLLM container...")
    # stop_vllm_container_if_needed()
    pass

# Finish
if run:
    run.finish()