#!/usr/bin/env python3
"""
JTruthfulQA Scoring Debug Script
JTruthfulQAã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãŒæ­¢ã¾ã‚‹å•é¡Œã‚’è¨ºæ–­ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import time
import torch
import psutil
import threading
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import subprocess
import signal

def monitor_resources():
    """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’ç›£è¦–"""
    while True:
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # GPUæƒ…å ±
            gpu_info = "N/A"
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                gpu_used = torch.cuda.memory_allocated(0)
                gpu_percent = (gpu_used / gpu_memory) * 100
                gpu_info = f"{gpu_percent:.1f}%"
            
            print(f"[MONITOR] CPU: {cpu_percent:.1f}% | Memory: {memory_percent:.1f}% | GPU: {gpu_info}")
            time.sleep(10)
        except Exception as e:
            print(f"[MONITOR ERROR] {e}")
            break

def test_jumanpp_direct():
    """Juman++ã®ç›´æ¥ãƒ†ã‚¹ãƒˆ"""
    print("=== Testing Juman++ directly ===")
    try:
        # ç’°å¢ƒå¤‰æ•°ç¢ºèª
        jumanpp_cmd = os.environ.get('JUMANPP_COMMAND', '/usr/local/bin/jumanpp')
        print(f"JUMANPP_COMMAND: {jumanpp_cmd}")
        
        # ç›´æ¥å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        result = subprocess.run([jumanpp_cmd, '--version'], 
                               capture_output=True, text=True, timeout=10)
        print(f"Version check: {result.returncode}")
        print(f"Output: {result.stdout.strip()}")
        
        # å®Ÿéš›ã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãƒ†ã‚¹ãƒˆ
        test_text = "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã€‚"
        process = subprocess.Popen([jumanpp_cmd], 
                                  stdin=subprocess.PIPE, 
                                  stdout=subprocess.PIPE, 
                                  stderr=subprocess.PIPE,
                                  text=True)
        
        stdout, stderr = process.communicate(input=test_text, timeout=10)
        print(f"Processing test: {process.returncode}")
        print(f"Output lines: {len(stdout.split())}")
        
    except subprocess.TimeoutExpired:
        print("âŒ Juman++ process timeout!")
        process.kill()
        return False
    except Exception as e:
        print(f"âŒ Juman++ error: {e}")
        return False
    
    print("âœ… Juman++ working normally")
    return True

def test_roberta_tokenizer():
    """RoBERTaãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== Testing RoBERTa Tokenizer ===")
    try:
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
        print("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained('nlp-waseda/roberta_jtruthfulqa')
        print("âœ… Tokenizer loaded")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        test_cases = [
            "ã“ã‚Œã¯çŸ­ã„ãƒ†ã‚¹ãƒˆã§ã™ã€‚",
            "ã“ã‚Œã¯ã‚ˆã‚Šé•·ã„ãƒ†ã‚¹ãƒˆã§ã™ã€‚æ—¥æœ¬èªã®è¤‡é›‘ãªæ–‡ç« ã‚’å‡¦ç†ã§ãã‚‹ã‹ãƒ†ã‚¹ãƒˆã—ã¦ã„ã¾ã™ã€‚",
            "è³ªå•: ã“ã‚Œã¯çœŸå®Ÿã§ã™ã‹ï¼Ÿ å›ç­”: ã¯ã„ã€ã“ã‚Œã¯äº‹å®Ÿã§ã™ã€‚",
        ]
        
        for i, text in enumerate(test_cases):
            print(f"Test case {i+1}: {text[:30]}...")
            start_time = time.time()
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            tokens = tokenizer.tokenize(text)
            elapsed = time.time() - start_time
            
            print(f"  Tokens: {len(tokens)}, Time: {elapsed:.3f}s")
            if elapsed > 5.0:
                print(f"  âš ï¸  Slow tokenization: {elapsed:.3f}s")
            
    except Exception as e:
        print(f"âŒ Tokenizer error: {e}")
        return False
    
    print("âœ… Tokenizer working normally")
    return True

def test_roberta_model():
    """RoBERTaãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== Testing RoBERTa Model ===")
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {device}")
        
        # GPUä½¿ç”¨çŠ¶æ³ãƒã‚§ãƒƒã‚¯
        if torch.cuda.is_available():
            print(f"GPU Memory before model load: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB")
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        print("Loading model...")
        tokenizer = AutoTokenizer.from_pretrained('nlp-waseda/roberta_jtruthfulqa')
        model = AutoModelForSequenceClassification.from_pretrained('nlp-waseda/roberta_jtruthfulqa').to(device)
        
        if torch.cuda.is_available():
            print(f"GPU Memory after model load: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB")
        
        # æ¨è«–ãƒ†ã‚¹ãƒˆ
        test_qa_pairs = [
            ("ã“ã‚Œã¯è³ªå•ã§ã™ã‹ï¼Ÿ", "ã¯ã„ã€ã“ã‚Œã¯è³ªå•ã§ã™ã€‚"),
            ("2+2ã¯4ã§ã™ã‹ï¼Ÿ", "ã¯ã„ã€2+2ã¯4ã§ã™ã€‚"),
            ("åœ°çƒã¯å¹³ã‚‰ã§ã™ã‹ï¼Ÿ", "ã„ã„ãˆã€åœ°çƒã¯çƒä½“ã§ã™ã€‚"),
        ]
        
        for i, (question, answer) in enumerate(test_qa_pairs):
            print(f"Test inference {i+1}...")
            start_time = time.time()
            
            # æ¨è«–å®Ÿè¡Œ
            inputs = tokenizer(question + " " + answer, return_tensors="pt", 
                             truncation=True, max_length=512).to(device)
            
            with torch.no_grad():
                outputs = model(**inputs)
            
            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
            score = probabilities[0][1].item()
            elapsed = time.time() - start_time
            
            print(f"  Score: {score:.4f}, Time: {elapsed:.3f}s")
            if elapsed > 10.0:
                print(f"  âš ï¸  Slow inference: {elapsed:.3f}s")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del model
        del tokenizer
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ Model error: {e}")
        return False
    
    print("âœ… Model working normally")
    return True

def test_batch_processing():
    """ãƒãƒƒãƒå‡¦ç†ã®ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ"""
    print("=== Testing Batch Processing ===")
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        tokenizer = AutoTokenizer.from_pretrained('nlp-waseda/roberta_jtruthfulqa')
        model = AutoModelForSequenceClassification.from_pretrained('nlp-waseda/roberta_jtruthfulqa').to(device)
        
        # 100å€‹ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ç”Ÿæˆ
        test_questions = [f"ã“ã‚Œã¯è³ªå•{i}ã§ã™ã‹ï¼Ÿ" for i in range(100)]
        test_answers = [f"ã¯ã„ã€ã“ã‚Œã¯å›ç­”{i}ã§ã™ã€‚" for i in range(100)]
        
        print(f"Processing {len(test_questions)} samples...")
        start_time = time.time()
        processed = 0
        
        for i, (question, answer) in enumerate(zip(test_questions, test_answers)):
            if i % 10 == 0:
                elapsed = time.time() - start_time
                rate = processed / elapsed if elapsed > 0 else 0
                print(f"  Progress: {i}/100, Rate: {rate:.2f} samples/sec")
            
            inputs = tokenizer(question + " " + answer, return_tensors="pt", 
                             truncation=True, max_length=512).to(device)
            
            with torch.no_grad():
                outputs = model(**inputs)
            
            processed += 1
            
            # ç•°å¸¸ã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å ´åˆã¯è­¦å‘Š
            if i > 0 and (time.time() - start_time) / i > 5.0:
                print(f"  âš ï¸  Slow processing detected: {(time.time() - start_time) / i:.3f}s per sample")
                break
        
        total_time = time.time() - start_time
        final_rate = processed / total_time
        print(f"Completed {processed} samples in {total_time:.2f}s ({final_rate:.2f} samples/sec)")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del model
        del tokenizer
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ Batch processing error: {e}")
        return False
    
    print("âœ… Batch processing completed")
    return True

def main():
    print("JTruthfulQA Scoring Debug Script")
    print("=" * 50)
    
    # ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§é–‹å§‹
    monitor_thread = threading.Thread(target=monitor_resources, daemon=True)
    monitor_thread.start()
    
    # å„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    tests = [
        ("Juman++ Direct Test", test_jumanpp_direct),
        ("RoBERTa Tokenizer Test", test_roberta_tokenizer),
        ("RoBERTa Model Test", test_roberta_model),
        ("Batch Processing Test", test_batch_processing),
    ]
    
    results = {}
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            results[test_name] = test_func()
        except KeyboardInterrupt:
            print(f"\nâŒ {test_name} interrupted by user")
            results[test_name] = False
            break
        except Exception as e:
            print(f"\nâŒ {test_name} failed with exception: {e}")
            results[test_name] = False
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\n{'='*20} Results Summary {'='*20}")
    for test_name, success in results.items():
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{test_name}: {status}")
    
    if all(results.values()):
        print("\nğŸ‰ All tests passed! JTruthfulQA should work normally.")
    else:
        print("\nâš ï¸  Some tests failed. Check the output above for details.")

if __name__ == "__main__":
    main() 